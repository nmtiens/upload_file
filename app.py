# Th√™m c√°c import c·∫ßn thi·∫øt ·ªü ƒë·∫ßu file
import hashlib
import json
import logging
import re
from flask import Flask, request, render_template, jsonify, send_file, abort
import os
from urllib.parse import unquote
from datetime import datetime
# S·ª≠ d·ª•ng: datetime.now()
from werkzeug.utils import secure_filename
import math
from pathlib import Path
import platform
import io
import uuid
from supabase import create_client, Client  
import zipfile
import shutil
from pathlib import Path

from utils import format_file_size, secure_folder_name
# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
# C·∫•u h√¨nh Supabase v·ªõi gi√° tr·ªã m·∫∑c ƒë·ªãnh v√† ki·ªÉm tra t·ªët h∆°n
SUPABASE_URL = os.environ.get('SUPABASE_URL', '')
SUPABASE_KEY = os.environ.get('SUPABASE_KEY', '')
SUPABASE_BUCKET = os.environ.get('SUPABASE_BUCKET', 'file-uploads')


# Ki·ªÉm tra v√† h∆∞·ªõng d·∫´n c·∫•u h√¨nh
if not SUPABASE_URL or not SUPABASE_KEY:
    print("=" * 60)
    print("‚ùå C·∫¢NH B√ÅO: Thi·∫øu c·∫•u h√¨nh Supabase!")
    print("=" * 60)
    print("Vui l√≤ng c·∫•u h√¨nh c√°c bi·∫øn m√¥i tr∆∞·ªùng sau:")
    print("")
    print("üîß C√ÅCH 1: S·ª≠ d·ª•ng file .env")
    print("T·∫°o file .env trong th∆∞ m·ª•c g·ªëc v·ªõi n·ªôi dung:")
    print("SUPABASE_URL=https://your-project-ref.supabase.co")
    print("SUPABASE_KEY=your-anon-public-key")
    print("SUPABASE_BUCKET=file-uploads")
    print("")
    print("üîß C√ÅCH 2: Set bi·∫øn m√¥i tr∆∞·ªùng (Windows)")
    print("set SUPABASE_URL=https://your-project-ref.supabase.co")
    print("set SUPABASE_KEY=your-anon-public-key")
    print("set SUPABASE_BUCKET=file-uploads")
    print("")
    print("üîß C√ÅCH 3: Set bi·∫øn m√¥i tr∆∞·ªùng (Linux/Mac)")
    print("export SUPABASE_URL=https://your-project-ref.supabase.co")
    print("export SUPABASE_KEY=your-anon-public-key")
    print("export SUPABASE_BUCKET=file-uploads")
    print("")
    print("üìã L·∫•y th√¥ng tin Supabase:")
    print("1. ƒêƒÉng nh·∫≠p v√†o https://supabase.com")
    print("2. Ch·ªçn project c·ªßa b·∫°n")
    print("3. V√†o Settings > API")
    print("4. Copy URL v√† anon/public key")
    print("")
    print("=" * 60)
    
    # Cho ph√©p ch·∫°y ·ªü ch·∫ø ƒë·ªô demo (kh√¥ng k·∫øt n·ªëi Supabase)
    print("üöÄ Kh·ªüi ƒë·ªông ·ªü ch·∫ø ƒë·ªô DEMO (kh√¥ng c√≥ Supabase)")
    print("·ª®ng d·ª•ng s·∫Ω ch·∫°y nh∆∞ng kh√¥ng th·ªÉ upload file th·∫≠t")
    print("=" * 60)
    
    # T·∫°o mock supabase client ƒë·ªÉ tr√°nh l·ªói
    class MockSupabase:
        def __init__(self):
            self.storage_instance = MockStorage()
            self.table_instance = MockTable()
        
        def storage(self):
            return self.storage_instance
        
        def table(self, table_name):
            return self.table_instance
    
    class MockStorage:
        def from_(self, bucket):
            return MockBucket()
    
    class MockBucket:
        def upload(self, path, file, file_options=None):
            return MockResult(200)
        
        def get_public_url(self, path):
            return f"https://demo.example.com/{path}"
        
        def list(self, path=""):
            return []
        
        def download(self, path):
            return b"Demo file content"
    
    class MockTable:
        def insert(self, data):
            return MockResult(200, data)
        
        def select(self, fields="*"):
            return self
        
        def eq(self, field, value):
            return self
        
        def single(self):
            return self
        
        def limit(self, count):
            return self
        
        def order(self, field, desc=False):
            return self
        
        def execute(self):
            return MockResult(200, [])
    
    class MockResult:
        def __init__(self, status_code, data=None):
            self.status_code = status_code
            self.data = data or []
    
    supabase = MockSupabase()
    DEMO_MODE = True
else:
    # Kh·ªüi t·∫°o Supabase client th·∫≠t
    try:
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        DEMO_MODE = False
        print(f"‚úÖ K·∫øt n·ªëi Supabase th√†nh c√¥ng!")
        print(f"üìç URL: {SUPABASE_URL}")
        print(f"üóÇÔ∏è Bucket: {SUPABASE_BUCKET}")
    except Exception as e:
        print(f"‚ùå L·ªói k·∫øt n·ªëi Supabase: {str(e)}")
        print("üîÑ Chuy·ªÉn sang ch·∫ø ƒë·ªô DEMO")
        # S·ª≠ d·ª•ng mock client
        DEMO_MODE = True
# Cache ƒë·ªÉ tr√°nh g·ªçi API nhi·ªÅu l·∫ßn

# C·∫•u h√¨nh app - T·ª™ 16MB l√™n 100MB ƒë·ªÉ h·ªó tr·ª£ file Word l·ªõn
app.config['MAX_CONTENT_LENGTH'] = 100 * 1024 * 1024  # 100MB max file size

# In th√¥ng tin kh·ªüi ƒë·ªông
print(f"=== FILE UPLOAD SERVER ===")
if DEMO_MODE:
    print("‚ö†Ô∏è  CH·∫†Y ·ªû CH·∫æ ƒê·ªò DEMO")
    print("üìù Ch·ª©c nƒÉng upload s·∫Ω m√¥ ph·ªèng")
else:
    print("‚úÖ CH·∫†Y V·ªöI SUPABASE")
    print(f"üåê Supabase URL: {SUPABASE_URL}")
    print(f"üóÇÔ∏è Bucket: {SUPABASE_BUCKET}")
print("=" * 30)

def init_db():
    """Kh·ªüi t·∫°o b·∫£ng submissions trong Supabase"""
    if DEMO_MODE:
        print("üìù Demo mode: B·ªè qua ki·ªÉm tra database")
        return
    
    try:
        # T·∫°o b·∫£ng submissions n·∫øu ch∆∞a c√≥
        result = supabase.table('submissions').select('*').limit(1).execute()
        print("‚úÖ Database connection successful!")
    except Exception as e:
        print(f"‚ùå Database initialization error: {str(e)}")
        print("üìã Vui l√≤ng t·∫°o b·∫£ng 'submissions' trong Supabase v·ªõi schema sau:")
        print("""
        CREATE TABLE submissions (
            id SERIAL PRIMARY KEY,
            ho_ten TEXT NOT NULL,
            noi_cong_tac TEXT,
            khoa_phong TEXT,
            ten_de_tai TEXT NOT NULL,
            gio_quy_doi REAL DEFAULT 0,
            minh_chung TEXT,
            ghi_chu TEXT,
            file_name TEXT,
            file_url TEXT,
            file_size INTEGER DEFAULT 0,
            folder_name TEXT,
            upload_time TIMESTAMP DEFAULT NOW(),
            upload_ip TEXT,
            storage_path TEXT
        );
        """)

init_db()

# M·ªü r·ªông ALLOWED_EXTENSIONS ƒë·ªÉ h·ªó tr·ª£ ƒë·∫ßy ƒë·ªß c√°c format Word v√† Office
ALLOWED_EXTENSIONS = {
    'txt', 'pdf', 'png', 'jpg', 'jpeg', 'gif', 'bmp', 'svg',
    'doc', 'docx', 'docm', 'dot', 'dotx', 'dotm',  # Word files
    'xls', 'xlsx', 'xlsm', 'xlsb', 'xlt', 'xltx', 'xltm',  # Excel files
    'ppt', 'pptx', 'pptm', 'pot', 'potx', 'potm',  # PowerPoint files
    'zip', 'rar', '7z', 'tar', 'gz',  # Archive files
    'mp3', 'mp4', 'avi', 'mov', 'wmv',  # Media files
    'csv', 'json', 'xml'  # Data files
}

def allowed_file(filename):
    """Ki·ªÉm tra file c√≥ ƒë∆∞·ª£c ph√©p upload kh√¥ng"""
    if not filename or '.' not in filename:
        return False
    
    extension = filename.rsplit('.', 1)[1].lower()
    is_allowed = extension in ALLOWED_EXTENSIONS
    
    # Log ƒë·ªÉ debug
    logger.info(f"Checking file: {filename}, extension: {extension}, allowed: {is_allowed}")
    
    return is_allowed

def get_content_type(filename):
    """X√°c ƒë·ªãnh content-type cho file"""
    if not filename:
        return "application/octet-stream"
    
    extension = filename.rsplit('.', 1)[1].lower() if '.' in filename else ""
    
    content_types = {
        # Microsoft Office
        'doc': 'application/msword',
        'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
        'docm': 'application/vnd.ms-word.document.macroEnabled.12',
        'dot': 'application/msword',
        'dotx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.template',
        'dotm': 'application/vnd.ms-word.template.macroEnabled.12',
        
        'xls': 'application/vnd.ms-excel',
        'xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
        'xlsm': 'application/vnd.ms-excel.sheet.macroEnabled.12',
        
        'ppt': 'application/vnd.ms-powerpoint',
        'pptx': 'application/vnd.openxmlformats-officedocument.presentationml.presentation',
        'pptm': 'application/vnd.ms-powerpoint.presentation.macroEnabled.12',
        
        # Other common types
        'pdf': 'application/pdf',
        'txt': 'text/plain',
        'csv': 'text/csv',
        'json': 'application/json',
        'xml': 'application/xml',
        
        # Images
        'jpg': 'image/jpeg',
        'jpeg': 'image/jpeg',
        'png': 'image/png',
        'gif': 'image/gif',
        'bmp': 'image/bmp',
        'svg': 'image/svg+xml',
        
        # Archives
        'zip': 'application/zip',
        'rar': 'application/x-rar-compressed',
        '7z': 'application/x-7z-compressed',
        
        # Media
        'mp3': 'audio/mpeg',
        'mp4': 'video/mp4',
        'avi': 'video/x-msvideo',
        'mov': 'video/quicktime',
    }
    
    return content_types.get(extension, "application/octet-stream")

def get_client_ip():
    """L·∫•y IP client"""
    if request.environ.get('HTTP_X_FORWARDED_FOR') is None:
        return request.environ['REMOTE_ADDR']
    else:
        return request.environ['HTTP_X_FORWARDED_FOR']

def upload_to_supabase(file, folder_name=None):
    """Upload file l√™n Supabase Storage"""
    if DEMO_MODE:
        # M√¥ ph·ªèng upload th√†nh c√¥ng
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        unique_id = str(uuid.uuid4())[:8]
        original_name = secure_filename(file.filename)
        name, ext = os.path.splitext(original_name)
        file_name = f"{name}_{timestamp}_{unique_id}{ext}"
        
        if folder_name:
            storage_path = f"{folder_name}/{file_name}"
        else:
            storage_path = file_name
        
        return {
            'success': True,
            'file_name': file_name,
            'storage_path': storage_path,
            'file_url': f"https://demo.example.com/{storage_path}",
            'file_size': 1024  # Gi·∫£ l·∫≠p 1KB
        }
    
    try:
        # T·∫°o t√™n file unique
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        unique_id = str(uuid.uuid4())[:8]
        original_name = secure_filename(file.filename)
        name, ext = os.path.splitext(original_name)
        file_name = f"{name}_{timestamp}_{unique_id}{ext}"
        
        # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n trong bucket
        if folder_name:
            storage_path = f"{folder_name}/{file_name}"
        else:
            storage_path = file_name
        
        # QUAN TR·ªåNG: Reset file pointer v·ªÅ ƒë·∫ßu tr∆∞·ªõc khi ƒë·ªçc
        file.seek(0)
        
        # ƒê·ªçc file content
        file_content = file.read()
        file_size = len(file_content)
        
        # X√°c ƒë·ªãnh content-type ch√≠nh x√°c
        content_type = get_content_type(original_name)
        
        logger.info(f"Uploading file: {file_name}, size: {file_size}, content-type: {content_type}")
        
        # Upload l√™n Supabase Storage
        result = supabase.storage.from_(SUPABASE_BUCKET).upload(
            path=storage_path,
            file=file_content,
            file_options={
                "content-type": content_type,
                "cache-control": "3600",
                "upsert": "false"  # Kh√¥ng ghi ƒë√® file c√πng t√™n
            }
        )
        
        # Ki·ªÉm tra k·∫øt qu·∫£ upload
        if hasattr(result, 'status_code'):
            status_code = result.status_code
        elif hasattr(result, 'data') and result.data:
            status_code = 200  # Th√†nh c√¥ng
        else:
            status_code = 400  # L·ªói
        
        if status_code == 200 or (hasattr(result, 'data') and result.data):
            # L·∫•y public URL
            public_url = supabase.storage.from_(SUPABASE_BUCKET).get_public_url(storage_path)
            
            logger.info(f"Upload successful: {storage_path}")
            
            return {
                'success': True,
                'file_name': file_name,
                'storage_path': storage_path,
                'file_url': public_url,
                'file_size': file_size
            }
        else:
            error_msg = f"Upload failed with status: {status_code}"
            if hasattr(result, 'error') and result.error:
                error_msg += f" - {result.error}"
            
            logger.error(error_msg)
            return {
                'success': False,
                'error': error_msg
            }
            
    except Exception as e:
        logger.error(f"Upload exception: {str(e)}")
        return {
            'success': False,
            'error': str(e)
        }

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/info')
def server_info():
    """API ƒë·ªÉ l·∫•y th√¥ng tin server"""
    storage_info = "Demo Mode (No Supabase)" if DEMO_MODE else "Supabase Storage"
    
    return jsonify({
        'storage_type': storage_info,
        'demo_mode': DEMO_MODE,
        'supabase_url': SUPABASE_URL if not DEMO_MODE else "Not configured",
        'supabase_bucket': SUPABASE_BUCKET,
        'server_platform': platform.system(),
        'allowed_extensions': list(ALLOWED_EXTENSIONS),
        'max_file_size': app.config['MAX_CONTENT_LENGTH'],
        'max_file_size_human': format_file_size(app.config['MAX_CONTENT_LENGTH'])
    })

@app.route('/upload', methods=['POST'])
def upload_file():
    try:
        # L·∫•y IP client
        client_ip = get_client_ip()
        
        # L·∫•y d·ªØ li·ªáu form
        ho_ten = request.form.get('ho_ten', '').strip()
        ten_de_tai = request.form.get('ten_de_tai', '').strip()
        noi_cong_tac = request.form.get('noi_cong_tac', '').strip()
        khoa_phong = request.form.get('khoa_phong', '').strip()
        gio_quy_doi = request.form.get('gio_quy_doi', '0')
        minh_chung = request.form.get('minh_chung', '').strip()
        ghi_chu = request.form.get('ghi_chu', '').strip()
        folder_name = request.form.get('folder', '').strip()

        # Ch·ªâ ki·ªÉm tra h·ªç t√™n v√† t√™n ƒë·ªÅ t√†i
        if not all([ho_ten, ten_de_tai]):
            return jsonify({'error': 'Vui l√≤ng ƒëi·ªÅn ƒë·∫ßy ƒë·ªß h·ªç t√™n v√† t√™n ƒë·ªÅ t√†i'}), 400

        try:
            gio_quy_doi = float(gio_quy_doi)
        except ValueError:
            gio_quy_doi = 0.0

        file_url = None
        file_name = None
        file_size = 0
        storage_path = None
        final_folder_name = None

        # X·ª≠ l√Ω folder name
        if folder_name:
            safe_folder_name = secure_folder_name(folder_name)
            if not safe_folder_name:
                return jsonify({'error': 'T√™n th∆∞ m·ª•c kh√¥ng h·ª£p l·ªá'}), 400
            final_folder_name = safe_folder_name

        # X·ª≠ l√Ω file upload
        if 'file' in request.files:
            file = request.files['file']
            
            # Ki·ªÉm tra file c√≥ t·ªìn t·∫°i v√† c√≥ t√™n kh√¥ng
            if not file or not file.filename or not file.filename.strip():
                logger.info("No file selected for upload")
            else:
                logger.info(f"Processing file: {file.filename}")
                
                if allowed_file(file.filename):
                    # Upload l√™n Supabase (ho·∫∑c m√¥ ph·ªèng)
                    upload_result = upload_to_supabase(file, final_folder_name)
                    
                    if upload_result['success']:
                        file_name = upload_result['file_name']
                        file_url = upload_result['file_url']
                        file_size = upload_result['file_size']
                        storage_path = upload_result['storage_path']
                        
                        status_text = "Demo upload" if DEMO_MODE else "Uploaded to Supabase"
                        logger.info(f"{status_text}: {storage_path} ({format_file_size(file_size)}) from IP: {client_ip}")
                    else:
                        logger.error(f"Upload failed: {upload_result['error']}")
                        return jsonify({'error': f'L·ªói upload: {upload_result["error"]}'}), 500
                else:
                    logger.warning(f"File type not allowed: {file.filename}")
                    return jsonify({'error': f'Lo·∫°i file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. C√°c ƒë·ªãnh d·∫°ng ƒë∆∞·ª£c h·ªó tr·ª£: {", ".join(sorted(ALLOWED_EXTENSIONS))}'}), 400

        # L∆∞u v√†o database (ho·∫∑c m√¥ ph·ªèng)
        try:
            submission_data = {
                'ho_ten': ho_ten,
                'ten_de_tai': ten_de_tai,
                'noi_cong_tac': noi_cong_tac,
                'khoa_phong': khoa_phong,
                'gio_quy_doi': gio_quy_doi,
                'minh_chung': minh_chung,
                'ghi_chu': ghi_chu,
                'file_name': file_name,
                'file_url': file_url,
                'file_size': file_size,
                'folder_name': final_folder_name,
                'upload_time': datetime.now().isoformat(),
                'upload_ip': client_ip,
                'storage_path': storage_path
            }
            
            if not DEMO_MODE:
                result = supabase.table('submissions').insert(submission_data).execute()
                logger.info(f"Data saved to Supabase database")
            else:
                logger.info(f"Demo mode - would save: {submission_data}")
            
        except Exception as e:
            logger.error(f"Database error: {str(e)}")
            if not DEMO_MODE:
                return jsonify({'error': f'L·ªói l∆∞u database: {str(e)}'}), 500

        # T·∫°o message ph·∫£n h·ªìi
        if DEMO_MODE:
            message = "Demo upload th√†nh c√¥ng! (Kh√¥ng c√≥ Supabase th·∫≠t)"
        else:
            if final_folder_name:
                message = f'ƒê√£ upload th√†nh c√¥ng v√†o th∆∞ m·ª•c "{final_folder_name}" tr√™n Supabase'
            else:
                message = 'Upload th√†nh c√¥ng l√™n Supabase Storage!'
        
        if file_url:
            message += f' - URL: {file_url}'

        return jsonify({
            'message': message,
            'file_name': file_name,
            'file_url': file_url,
            'file_size': file_size,
            'file_size_human': format_file_size(file_size) if file_size else "0 B",
            'folder': final_folder_name,
            'storage_path': storage_path,
            'client_ip': client_ip,
            'demo_mode': DEMO_MODE
        })

    except Exception as e:
        logger.error(f"Upload error: {str(e)}")
        return jsonify({'error': f'L·ªói: {str(e)}'}), 500
    
    
def get_all_storage_files(supabase, bucket_name, path="", max_files=5000):
    """
    L·∫•y t·∫•t c·∫£ files trong storage m·ªôt c√°ch recursive
    """
    all_files = []
    folders_to_process = [path] if path else [""]
    processed_folders = set()
    
    while folders_to_process:
        current_path = folders_to_process.pop(0)
        
        # Tr√°nh x·ª≠ l√Ω folder tr√πng l·∫∑p
        if current_path in processed_folders:
            continue
        processed_folders.add(current_path)
        
        try:
            # L·∫•y items trong folder hi·ªán t·∫°i
            items = supabase.storage.from_(bucket_name).list(
                path=current_path, 
                options={"limit": 1000}
            )
            
            if not items:
                continue
                
            for item in items:
                item_name = item.get('name', '')
                if not item_name:
                    continue
                    
                # T·∫°o full path
                if current_path:
                    full_path = f"{current_path}/{item_name}"
                else:
                    full_path = item_name
                
                # Ki·ªÉm tra xem ƒë√¢y l√† file hay folder
                # Folder th∆∞·ªùng c√≥ size = 0 ho·∫∑c None v√† updated_at = None
                metadata = item.get('metadata', {}) or {}
                file_size = metadata.get('size', 0) or 0
                updated_at = item.get('updated_at')
                
                # N·∫øu l√† folder (size = 0 v√† kh√¥ng c√≥ updated_at)
                if file_size == 0 and updated_at is None:
                    # Th√™m folder v√†o queue ƒë·ªÉ x·ª≠ l√Ω
                    folders_to_process.append(full_path)
                else:
                    # ƒê√¢y l√† file th·∫≠t
                    all_files.append({
                        'name': item_name,
                        'full_path': full_path,
                        'folder': current_path,
                        'size': file_size,
                        'updated_at': updated_at,
                        'metadata': item
                    })
                
                # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng files ƒë·ªÉ tr√°nh timeout
                if len(all_files) >= max_files:
                    break
            
            if len(all_files) >= max_files:
                break
                
        except Exception as e:
            print(f"L·ªói khi l·∫•y files t·ª´ folder '{current_path}': {str(e)}")
            continue
    
    return all_files

# API ƒë·ªÉ l·∫•y preview c·∫•u tr√∫c th∆∞ m·ª•c tr∆∞·ªõc khi download (FIXED)
@app.route('/api/preview/storage-structure', methods=['GET'])
def preview_storage_structure():
    """
    API ƒë·ªÉ xem preview c·∫•u tr√∫c storage tr∆∞·ªõc khi download
    """
    try:
        if DEMO_MODE:
            return jsonify({
                'success': False,
                'error': 'T√≠nh nƒÉng n√†y kh√¥ng kh·∫£ d·ª•ng ·ªü Demo Mode'
            }), 400
        
        print("üîç ƒêang qu√©t c·∫•u tr√∫c storage...")
        
        # L·∫•y t·∫•t c·∫£ files t·ª´ storage
        all_files = get_all_storage_files(supabase, SUPABASE_BUCKET)
        
        if not all_files:
            return jsonify({
                'success': True,
                'message': 'Kh√¥ng t√¨m th·∫•y files n√†o trong storage',
                'data': {
                    'folders': {},
                    'root_files': [],
                    'statistics': {
                        'total_folders': 0,
                        'total_files': 0,
                        'total_size': 0,
                        'total_size_human': '0 B'
                    }
                }
            })
        
        # Ph√¢n t√≠ch c·∫•u tr√∫c
        folders_structure = {}
        root_files = []
        total_size = 0
        
        for file_info in all_files:
            file_name = file_info['name']
            full_path = file_info['full_path']
            folder = file_info['folder']
            file_size = file_info['size']
            updated_at = file_info['updated_at']
            
            total_size += file_size
            
            if folder and folder != "":
                # File trong subfolder
                if folder not in folders_structure:
                    folders_structure[folder] = {
                        'name': folder,
                        'files': [],
                        'file_count': 0,
                        'total_size': 0
                    }
                
                folders_structure[folder]['files'].append({
                    'name': file_name,
                    'full_path': full_path,
                    'size': file_size,
                    'size_human': format_file_size(file_size),
                    'updated_at': updated_at
                })
                folders_structure[folder]['file_count'] += 1
                folders_structure[folder]['total_size'] += file_size
                
            else:
                # File ·ªü root
                root_files.append({
                    'name': file_name,
                    'full_path': full_path,
                    'size': file_size,
                    'size_human': format_file_size(file_size),
                    'updated_at': updated_at
                })
        
        # Format folders
        for folder_name, folder_info in folders_structure.items():
            folder_info['total_size_human'] = format_file_size(folder_info['total_size'])
        
        # T√≠nh to√°n th·ªëng k√™
        total_files = len(all_files)
        
        print(f"‚úÖ Qu√©t ho√†n th√†nh: {total_files} files trong {len(folders_structure)} folders")
        
        return jsonify({
            'success': True,
            'data': {
                'folders': folders_structure,
                'root_files': root_files,
                'statistics': {
                    'total_folders': len(folders_structure),
                    'total_files': total_files,
                    'total_size': total_size,
                    'total_size_human': format_file_size(total_size),
                    'folders_with_files': len([f for f in folders_structure.values() if f['file_count'] > 0]),
                    'root_file_count': len(root_files)
                }
            }
        })
        
    except Exception as e:
        print(f"‚ùå L·ªói l·∫•y c·∫•u tr√∫c storage: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'L·ªói l·∫•y c·∫•u tr√∫c storage: {str(e)}'
        }), 500
@app.route('/api/download/all-folders', methods=['GET', 'POST'])
def download_all_folders():
    """
    API ƒë·ªÉ download t·∫•t c·∫£ folders v√† files t·ª´ Supabase Storage v·ªÅ m√°y local
    H·ªó tr·ª£ incremental sync - ch·ªâ t·∫£i file m·ªõi/thay ƒë·ªïi t·ª´ l·∫ßn 2
    
    Query Parameters:
    - format: 'zip' ho·∫∑c 'folders' (default: 'folders')
    - path: ƒë∆∞·ªùng d·∫´n l∆∞u local (default: './downloads')
    - include_metadata: true/false - c√≥ l∆∞u metadata kh√¥ng (default: true)
    - force_full: true/false - bu·ªôc download to√†n b·ªô (default: false)
    - sync_mode: 'incremental' ho·∫∑c 'full' (default: 'incremental')
    """
    try:
        if DEMO_MODE:
            return jsonify({
                'success': False,
                'error': 'T√≠nh nƒÉng n√†y kh√¥ng kh·∫£ d·ª•ng ·ªü Demo Mode'
            }), 400
        
        # L·∫•y parameters
        download_format = request.args.get('format', 'folders').lower()
        local_path = request.args.get('path', './downloads')
        include_metadata = request.args.get('include_metadata', 'true').lower() == 'true'
        force_full = request.args.get('force_full', 'false').lower() == 'true'
        sync_mode = request.args.get('sync_mode', 'incremental').lower()
        
        # T·∫°o th∆∞ m·ª•c download n·∫øu ch∆∞a c√≥
        download_dir = Path(local_path)
        download_dir.mkdir(parents=True, exist_ok=True)
        
        # ƒê∆∞·ªùng d·∫´n file sync state
        sync_state_file = download_dir / '.sync_state.json'
        
        print(f"üöÄ B·∫Øt ƒë·∫ßu {'full' if force_full or sync_mode == 'full' else 'incremental'} download...")
        print(f"üìÅ L∆∞u t·∫°i: {download_dir.absolute()}")
        print(f"üì¶ Format: {download_format}")
        
        # ƒê·ªçc sync state t·ª´ l·∫ßn download tr∆∞·ªõc (n·∫øu c√≥)
        previous_sync_state = {}
        is_first_sync = True
        
        if sync_state_file.exists() and not force_full and sync_mode == 'incremental':
            try:
                with open(sync_state_file, 'r', encoding='utf-8') as f:
                    previous_sync_state = json.load(f)
                is_first_sync = False
                print(f"üìã T√¨m th·∫•y sync state t·ª´ l·∫ßn tr∆∞·ªõc: {previous_sync_state.get('last_sync', 'N/A')}")
            except Exception as e:
                print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ ƒë·ªçc sync state: {e}. S·∫Ω th·ª±c hi·ªán full sync.")
                is_first_sync = True
        
        # L·∫•y t·∫•t c·∫£ files t·ª´ storage
        try:
            all_files = get_all_storage_files(supabase, SUPABASE_BUCKET)
            
            if not all_files:
                return jsonify({
                    'success': False,
                    'error': 'Kh√¥ng t√¨m th·∫•y file n√†o trong storage'
                }), 404
            
            print(f"üìä T√¨m th·∫•y {len(all_files)} files tr√™n storage")
            
        except Exception as e:
            return jsonify({
                'success': False,
                'error': f'L·ªói l·∫•y danh s√°ch files: {str(e)}'
            }), 500
        
        # L·∫•y danh s√°ch folders t·ª´ storage ƒë·ªÉ t·∫°o c·∫•u tr√∫c th∆∞ m·ª•c
        folders_on_storage = set()
        files_to_process = []
        
        # Ph√¢n lo·∫°i files v√† x√°c ƒë·ªãnh files c·∫ßn download
        for file_info in all_files:
            folder = file_info['folder']
            file_path = file_info['full_path']
            
            # Th√™m folder v√†o danh s√°ch
            if folder and folder != "":
                folders_on_storage.add(folder)
                # Th√™m c√°c parent folders n·∫øu c√≥ nested structure
                folder_parts = folder.split('/')
                for i in range(1, len(folder_parts) + 1):
                    parent_folder = '/'.join(folder_parts[:i])
                    folders_on_storage.add(parent_folder)
            
            # Ki·ªÉm tra xem file c√≥ c·∫ßn download kh√¥ng
            should_download = True
            
            if not is_first_sync and sync_mode == 'incremental':
                # So s√°nh v·ªõi sync state tr∆∞·ªõc
                previous_file_info = previous_sync_state.get('files', {}).get(file_path)
                
                if previous_file_info:
                    # So s√°nh updated_at v√† size
                    current_updated = file_info.get('updated_at', '')
                    previous_updated = previous_file_info.get('updated_at', '')
                    current_size = file_info.get('size', 0)
                    previous_size = previous_file_info.get('size', 0)
                    
                    if (current_updated == previous_updated and 
                        current_size == previous_size):
                        # File kh√¥ng thay ƒë·ªïi, ki·ªÉm tra xem file local c√≥ t·ªìn t·∫°i kh√¥ng
                        if folder:
                            local_file_path = download_dir / folder / file_info['name']
                        else:
                            local_file_path = download_dir / file_info['name']
                        
                        if local_file_path.exists():
                            should_download = False
                            print(f"‚è≠Ô∏è Skip unchanged file: {file_path}")
            
            if should_download:
                files_to_process.append(file_info)
        
        print(f"üìÅ T√¨m th·∫•y {len(folders_on_storage)} folders tr√™n storage")
        print(f"üìÑ C·∫ßn download {len(files_to_process)} files")
        
        # T·∫°o t·∫•t c·∫£ folders tr∆∞·ªõc khi download
        print("üèóÔ∏è T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c...")
        for folder_name in sorted(folders_on_storage):
            folder_path = download_dir / folder_name
            folder_path.mkdir(parents=True, exist_ok=True)
            print(f"   üìÅ Created: {folder_path}")
        
        # N·∫øu kh√¥ng c√≥ file n√†o c·∫ßn download
        if not files_to_process:
            print("‚úÖ T·∫•t c·∫£ files ƒë√£ ƒë∆∞·ª£c sync, kh√¥ng c√≥ g√¨ ƒë·ªÉ download!")
            
            return jsonify({
                'success': True,
                'message': 'T·∫•t c·∫£ files ƒë√£ ƒë∆∞·ª£c sync, kh√¥ng c√≥ file m·ªõi ƒë·ªÉ download',
                'data': {
                    'download_path': str(download_dir.absolute()),
                    'sync_type': 'incremental',
                    'statistics': {
                        'total_folders': len(folders_on_storage),
                        'total_files_on_storage': len(all_files),
                        'files_to_download': 0,
                        'already_synced': len(all_files),
                        'total_size': 0
                    }
                }
            })
        
        # Ph√¢n lo·∫°i files c·∫ßn download theo folders
        folders_structure = {}
        root_files = []
        
        for file_info in files_to_process:
            folder = file_info['folder']
            
            if folder and folder != "":
                if folder not in folders_structure:
                    folders_structure[folder] = []
                folders_structure[folder].append(file_info)
            else:
                root_files.append(file_info)
        
        download_results = {
            'folders': {},
            'root_files': [],
            'total_files': 0,
            'total_size': 0,
            'errors': [],
            'skipped_files': len(all_files) - len(files_to_process)
        }
        
        # Function ƒë·ªÉ t√≠nh checksum c·ªßa file
        def calculate_file_checksum(file_path):
            try:
                hash_md5 = hashlib.md5()
                with open(file_path, "rb") as f:
                    for chunk in iter(lambda: f.read(4096), b""):
                        hash_md5.update(chunk)
                return hash_md5.hexdigest()
            except:
                return None
        
        # Function ƒë·ªÉ download m·ªôt file
        def download_single_file(file_info, local_file_path):
            try:
                storage_path = file_info['full_path']
                
                # Download file t·ª´ Supabase
                file_data = supabase.storage.from_(SUPABASE_BUCKET).download(storage_path)
                
                if not file_data:
                    return {
                        'success': False,
                        'error': f'Kh√¥ng th·ªÉ download {storage_path}',
                        'path': storage_path
                    }
                
                # T·∫°o th∆∞ m·ª•c n·∫øu c·∫ßn (ƒë√£ t·∫°o tr∆∞·ªõc ƒë√≥ nh∆∞ng ƒë·∫£m b·∫£o)
                local_file_path.parent.mkdir(parents=True, exist_ok=True)
                
                # Ghi file
                with open(local_file_path, 'wb') as f:
                    f.write(file_data)
                
                file_size = len(file_data)
                
                # T√≠nh checksum ƒë·ªÉ verify
                checksum = calculate_file_checksum(local_file_path)
                
                return {
                    'success': True,
                    'storage_path': storage_path,
                    'local_path': str(local_file_path),
                    'size': file_size,
                    'checksum': checksum,
                    'metadata': file_info
                }
                
            except Exception as e:
                return {
                    'success': False,
                    'error': str(e),
                    'path': file_info['full_path']
                }
        
        # T·∫°o current sync state ƒë·ªÉ l∆∞u
        current_sync_state = {
            'last_sync': datetime.now().isoformat(),
            'sync_type': 'full' if is_first_sync else 'incremental',
            'total_files_on_storage': len(all_files),
            'files_downloaded': 0,
            'files': {}
        }
        
        # Download files theo format
        if download_format == 'zip':
            # T·∫°o file zip v·ªõi timestamp
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            sync_type = 'full' if is_first_sync else 'incremental'
            zip_filename = f"supabase_storage_{sync_type}_{timestamp}.zip"
            zip_path = download_dir / zip_filename
            
            # T·∫°o th∆∞ m·ª•c temp
            temp_dir = download_dir / 'temp'
            temp_dir.mkdir(exist_ok=True)
            
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                
                # Download files trong folders
                for folder_name, files in folders_structure.items():
                    print(f"üìÅ Processing folder: {folder_name} ({len(files)} files)")
                    
                    folder_results = {
                        'name': folder_name,
                        'files': [],
                        'total_files': len(files),
                        'success_count': 0,
                        'error_count': 0
                    }
                    
                    for file_info in files:
                        temp_file_path = temp_dir / file_info['full_path']
                        result = download_single_file(file_info, temp_file_path)
                        
                        if result['success']:
                            # Th√™m v√†o zip v·ªõi ƒë√∫ng c·∫•u tr√∫c th∆∞ m·ª•c
                            zipf.write(temp_file_path, file_info['full_path'])
                            
                            folder_results['files'].append(result)
                            folder_results['success_count'] += 1
                            download_results['total_files'] += 1
                            download_results['total_size'] += result['size']
                            
                            # C·∫≠p nh·∫≠t sync state
                            current_sync_state['files'][file_info['full_path']] = {
                                'updated_at': file_info.get('updated_at'),
                                'size': file_info.get('size'),
                                'checksum': result['checksum']
                            }
                            
                            # X√≥a file temp
                            temp_file_path.unlink()
                            
                        else:
                            folder_results['error_count'] += 1
                            download_results['errors'].append(result)
                    
                    download_results['folders'][folder_name] = folder_results
                
                # Download root files
                if root_files:
                    print(f"üìÑ Processing root files: {len(root_files)} files")
                    
                    for file_info in root_files:
                        temp_file_path = temp_dir / file_info['name']
                        result = download_single_file(file_info, temp_file_path)
                        
                        if result['success']:
                            zipf.write(temp_file_path, file_info['name'])
                            
                            download_results['root_files'].append(result)
                            download_results['total_files'] += 1
                            download_results['total_size'] += result['size']
                            
                            # C·∫≠p nh·∫≠t sync state
                            current_sync_state['files'][file_info['full_path']] = {
                                'updated_at': file_info.get('updated_at'),
                                'size': file_info.get('size'),
                                'checksum': result['checksum']
                            }
                            
                            # X√≥a file temp
                            temp_file_path.unlink()
                            
                        else:
                            download_results['errors'].append(result)
                
                # Th√™m metadata file n·∫øu c·∫ßn
                if include_metadata:
                    metadata_content = {
                        'download_info': {
                            'timestamp': datetime.now().isoformat(),
                            'sync_type': current_sync_state['sync_type'],
                            'bucket': SUPABASE_BUCKET,
                            'total_folders': len(folders_on_storage),
                            'files_on_storage': len(all_files),
                            'files_downloaded': download_results['total_files'],
                            'files_skipped': download_results['skipped_files'],
                            'total_size': download_results['total_size'],
                            'total_size_human': format_file_size(download_results['total_size'])
                        },
                        'folders_structure': list(folders_on_storage),
                        'downloaded_folders': folders_structure,
                        'downloaded_root_files': root_files
                    }
                    
                    metadata_path = temp_dir / 'metadata.json'
                    with open(metadata_path, 'w', encoding='utf-8') as f:
                        json.dump(metadata_content, f, indent=2, ensure_ascii=False, default=str)
                    
                    zipf.write(metadata_path, 'metadata.json')
                    metadata_path.unlink()
            
            # X√≥a th∆∞ m·ª•c temp
            if temp_dir.exists():
                shutil.rmtree(temp_dir)
            
            download_results['zip_file'] = str(zip_path)
            download_results['zip_size'] = zip_path.stat().st_size
            download_results['zip_size_human'] = format_file_size(zip_path.stat().st_size)
            
        else:
            # Download th√†nh folders ri√™ng bi·ªát
            
            # Download files trong folders
            for folder_name, files in folders_structure.items():
                print(f"üìÅ Downloading folder: {folder_name} ({len(files)} files)")
                
                folder_path = download_dir / folder_name
                # Folder ƒë√£ ƒë∆∞·ª£c t·∫°o ·ªü tr√™n
                
                folder_results = {
                    'name': folder_name,
                    'path': str(folder_path),
                    'files': [],
                    'total_files': len(files),
                    'success_count': 0,
                    'error_count': 0
                }
                
                for file_info in files:
                    local_file_path = folder_path / file_info['name']
                    
                    result = download_single_file(file_info, local_file_path)
                    
                    if result['success']:
                        folder_results['files'].append(result)
                        folder_results['success_count'] += 1
                        download_results['total_files'] += 1
                        download_results['total_size'] += result['size']
                        
                        # C·∫≠p nh·∫≠t sync state
                        current_sync_state['files'][file_info['full_path']] = {
                            'updated_at': file_info.get('updated_at'),
                            'size': file_info.get('size'),
                            'checksum': result['checksum']
                        }
                    else:
                        folder_results['error_count'] += 1
                        download_results['errors'].append(result)
                
                download_results['folders'][folder_name] = folder_results
            
            # Download root files
            if root_files:
                print(f"üìÑ Downloading root files: {len(root_files)} files")
                
                for file_info in root_files:
                    local_file_path = download_dir / file_info['name']
                    
                    result = download_single_file(file_info, local_file_path)
                    
                    if result['success']:
                        download_results['root_files'].append(result)
                        download_results['total_files'] += 1
                        download_results['total_size'] += result['size']
                        
                        # C·∫≠p nh·∫≠t sync state
                        current_sync_state['files'][file_info['full_path']] = {
                            'updated_at': file_info.get('updated_at'),
                            'size': file_info.get('size'),
                            'checksum': result['checksum']
                        }
                    else:
                        download_results['errors'].append(result)
        
        # C·∫≠p nh·∫≠t sync state v·ªõi files t·ª´ previous state (files kh√¥ng thay ƒë·ªïi)
        if not is_first_sync:
            for file_path, file_data in previous_sync_state.get('files', {}).items():
                if file_path not in current_sync_state['files']:
                    # File n√†y kh√¥ng ƒë∆∞·ª£c download l·∫ßn n√†y (kh√¥ng thay ƒë·ªïi)
                    current_sync_state['files'][file_path] = file_data
        
        current_sync_state['files_downloaded'] = download_results['total_files']
        
        # L∆∞u sync state m·ªõi
        try:
            with open(sync_state_file, 'w', encoding='utf-8') as f:
                json.dump(current_sync_state, f, indent=2, ensure_ascii=False, default=str)
            print(f"üíæ ƒê√£ l∆∞u sync state t·∫°i: {sync_state_file}")
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ l∆∞u sync state: {e}")
        
        # T·∫°o metadata file n·∫øu c·∫ßn (cho folder mode)
        if include_metadata and download_format != 'zip':
            metadata_content = {
                'download_info': {
                    'timestamp': datetime.now().isoformat(),
                    'sync_type': current_sync_state['sync_type'],
                    'bucket': SUPABASE_BUCKET,
                    'total_folders': len(folders_on_storage),
                    'files_on_storage': len(all_files),
                    'files_downloaded': download_results['total_files'],
                    'files_skipped': download_results['skipped_files'],
                    'total_size': download_results['total_size'],
                    'total_size_human': format_file_size(download_results['total_size'])
                },
                'folders_structure': list(folders_on_storage),
                'download_results': download_results
            }
            
            metadata_path = download_dir / 'download_metadata.json'
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata_content, f, indent=2, ensure_ascii=False, default=str)
            
            download_results['metadata_file'] = str(metadata_path)
        
        # T√≠nh to√°n th·ªëng k√™ cu·ªëi
        success_count = download_results['total_files']
        error_count = len(download_results['errors'])
        skipped_count = download_results['skipped_files']
        
        print(f"‚úÖ {'Full' if is_first_sync else 'Incremental'} sync ho√†n th√†nh!")
        print(f"üìä Th·ªëng k√™:")
        print(f"   - T·ªïng folders: {len(folders_on_storage)}")
        print(f"   - Files tr√™n storage: {len(all_files)}")
        print(f"   - Files downloaded: {success_count}")
        print(f"   - Files skipped: {skipped_count}")
        print(f"   - Files l·ªói: {error_count}")
        print(f"   - T·ªïng dung l∆∞·ª£ng downloaded: {format_file_size(download_results['total_size'])}")
        
        return jsonify({
            'success': True,
            'message': f'{"Full" if is_first_sync else "Incremental"} sync ho√†n th√†nh: {success_count} files downloaded, {skipped_count} files skipped',
            'data': {
                'download_path': str(download_dir.absolute()),
                'format': download_format,
                'sync_type': current_sync_state['sync_type'],
                'statistics': {
                    'total_folders': len(folders_on_storage),
                    'files_on_storage': len(all_files),
                    'files_downloaded': success_count,
                    'files_skipped': skipped_count,
                    'files_failed': error_count,
                    'total_size': download_results['total_size'],
                    'total_size_human': format_file_size(download_results['total_size'])
                },
                'results': download_results
            }
        })
        
    except Exception as e:
        print(f"‚ùå L·ªói download folders: {str(e)}")
        import traceback
        traceback.print_exc()
        
        return jsonify({
            'success': False,
            'error': f'L·ªói download: {str(e)}'
        }), 500


# Helper function ƒë·ªÉ l·∫•y danh s√°ch t·∫•t c·∫£ folders t·ª´ storage
def get_all_folders_from_storage(supabase_client, bucket_name):
    """
    L·∫•y danh s√°ch t·∫•t c·∫£ folders t·ª´ Supabase Storage
    """
    try:
        # L·∫•y t·∫•t c·∫£ files
        result = supabase_client.storage.from_(bucket_name).list()
        
        folders = set()
        
        def extract_folders_recursive(items, current_path=""):
            for item in items:
                item_name = item.get('name', '')
                
                if current_path:
                    full_path = f"{current_path}/{item_name}"
                else:
                    full_path = item_name
                
                # N·∫øu l√† folder (kh√¥ng c√≥ metadata file info)
                if item.get('metadata') is None or 'size' not in item.get('metadata', {}):
                    folders.add(full_path)
                    
                    # Recursively list contents of this folder
                    try:
                        subfolder_result = supabase_client.storage.from_(bucket_name).list(full_path)
                        extract_folders_recursive(subfolder_result, full_path)
                    except:
                        pass  # Ignore errors when listing subfolders
                else:
                    # File - extract its parent folder
                    if '/' in full_path:
                        parent_folder = '/'.join(full_path.split('/')[:-1])
                        folders.add(parent_folder)
        
        extract_folders_recursive(result)
        
        return list(folders)
        
    except Exception as e:
        print(f"Error getting folders: {e}")
        return []


# API ƒë·ªÉ l·∫•y danh s√°ch folders
@app.route('/api/storage/folders', methods=['GET'])
def get_storage_folders():
    """
    API ƒë·ªÉ l·∫•y danh s√°ch t·∫•t c·∫£ folders trong Supabase Storage
    """
    try:
        if DEMO_MODE:
            return jsonify({
                'success': False,
                'error': 'T√≠nh nƒÉng n√†y kh√¥ng kh·∫£ d·ª•ng ·ªü Demo Mode'
            }), 400
        
        folders = get_all_folders_from_storage(supabase, SUPABASE_BUCKET)
        
        return jsonify({
            'success': True,
            'data': {
                'folders': sorted(folders),
                'total_folders': len(folders)
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'L·ªói l·∫•y danh s√°ch folders: {str(e)}'
        }), 500

# Thay th·∫ø 2 API functions n√†y trong file ch√≠nh

# API ƒë·ªÉ rename th∆∞ m·ª•c - FIXED VERSION
@app.route('/api/storage/rename-folder', methods=['POST'])
def rename_folder():
    """
    API ƒë·ªÉ ƒë·ªïi t√™n th∆∞ m·ª•c trong storage - Fixed version
    """
    try:
        if DEMO_MODE:
            return jsonify({
                'success': False,
                'error': 'T√≠nh nƒÉng n√†y kh√¥ng kh·∫£ d·ª•ng ·ªü Demo Mode'
            }), 400
        
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng c√≥ d·ªØ li·ªáu JSON'
            }), 400
            
        old_folder_name = data.get('old_folder_name', '').strip()
        new_folder_name = data.get('new_folder_name', '').strip()
        
        if not old_folder_name or not new_folder_name:
            return jsonify({
                'success': False,
                'error': 'T√™n th∆∞ m·ª•c c≈© v√† m·ªõi kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng'
            }), 400
        
        if old_folder_name == new_folder_name:
            return jsonify({
                'success': False,
                'error': 'T√™n th∆∞ m·ª•c m·ªõi ph·∫£i kh√°c v·ªõi t√™n c≈©'
            }), 400
        
        # Validate t√™n folder m·ªõi
        if '/' in new_folder_name or '\\' in new_folder_name:
            return jsonify({
                'success': False,
                'error': 'T√™n th∆∞ m·ª•c kh√¥ng ƒë∆∞·ª£c ch·ª©a k√Ω t·ª± / ho·∫∑c \\'
            }), 400
        
        print(f"üîÑ ƒêang rename folder '{old_folder_name}' ‚Üí '{new_folder_name}'...")
        
        # L·∫•y t·∫•t c·∫£ files trong folder c≈©
        all_files = get_all_storage_files(supabase, SUPABASE_BUCKET)
        folder_files = [f for f in all_files if f['folder'] == old_folder_name]
        
        if not folder_files:
            return jsonify({
                'success': False,
                'error': f'Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c "{old_folder_name}" ho·∫∑c th∆∞ m·ª•c r·ªóng'
            }), 404
        
        # Ki·ªÉm tra folder m·ªõi ƒë√£ t·ªìn t·∫°i ch∆∞a
        existing_new_folder = [f for f in all_files if f['folder'] == new_folder_name]
        if existing_new_folder:
            return jsonify({
                'success': False,
                'error': f'Th∆∞ m·ª•c "{new_folder_name}" ƒë√£ t·ªìn t·∫°i'
            }), 400
        
        # Th·ª±c hi·ªán copy + delete cho t·ª´ng file (v√¨ Supabase kh√¥ng c√≥ move tr·ª±c ti·∫øp)
        moved_files = []
        failed_files = []
        
        for file_info in folder_files:
            old_path = file_info['full_path']
            new_path = f"{new_folder_name}/{file_info['name']}"
            
            try:
                print(f"  üìÑ Moving: {old_path} ‚Üí {new_path}")
                
                # B∆∞·ªõc 1: Download file content
                download_response = supabase.storage.from_(SUPABASE_BUCKET).download(old_path)
                
                if not download_response:
                    failed_files.append({
                        'file': file_info['name'],
                        'error': 'Kh√¥ng th·ªÉ download file t·ª´ path c≈©'
                    })
                    continue
                
                # B∆∞·ªõc 2: Upload v·ªõi path m·ªõi
                upload_response = supabase.storage.from_(SUPABASE_BUCKET).upload(
                    path=new_path,
                    file=download_response,
                    file_options={
                        "content-type": file_info.get('metadata', {}).get('content-type', 'application/octet-stream')
                    }
                )
                
                # Ki·ªÉm tra upload th√†nh c√¥ng
                if hasattr(upload_response, 'error') and upload_response.error:
                    failed_files.append({
                        'file': file_info['name'],
                        'error': f'Upload failed: {upload_response.error}'
                    })
                    continue
                
                # B∆∞·ªõc 3: X√≥a file c≈©
                delete_response = supabase.storage.from_(SUPABASE_BUCKET).remove([old_path])
                
                # Ki·ªÉm tra x√≥a th√†nh c√¥ng
                if hasattr(delete_response, 'error') and delete_response.error:
                    print(f"‚ö†Ô∏è Warning: Couldn't delete old file {old_path}: {delete_response.error}")
                    # Kh√¥ng fail to√†n b·ªô operation v√¨ file m·ªõi ƒë√£ ƒë∆∞·ª£c t·∫°o
                
                moved_files.append({
                    'old_path': old_path,
                    'new_path': new_path,
                    'file_name': file_info['name']
                })
                    
            except Exception as e:
                failed_files.append({
                    'file': file_info['name'],
                    'error': str(e)
                })
                print(f"‚ùå Error moving {file_info['name']}: {str(e)}")
        
        if len(failed_files) == len(folder_files):
            # T·∫•t c·∫£ files ƒë·ªÅu fail
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng th·ªÉ move b·∫•t k·ª≥ file n√†o',
                'details': {
                    'failed_files': failed_files
                }
            }), 500
        
        success_count = len(moved_files)
        total_count = len(folder_files)
        
        print(f"‚úÖ Rename folder: {success_count}/{total_count} files moved successfully")
        
        # N·∫øu c√≥ m·ªôt s·ªë files fail nh∆∞ng kh√¥ng ph·∫£i t·∫•t c·∫£
        if failed_files:
            return jsonify({
                'success': True,
                'message': f'ƒê√£ ƒë·ªïi t√™n th∆∞ m·ª•c "{old_folder_name}" th√†nh "{new_folder_name}" ({success_count}/{total_count} files)',
                'warning': f'{len(failed_files)} files kh√¥ng th·ªÉ move',
                'data': {
                    'old_folder_name': old_folder_name,
                    'new_folder_name': new_folder_name,
                    'moved_files_count': success_count,
                    'failed_files_count': len(failed_files),
                    'moved_files': moved_files,
                    'failed_files': failed_files
                }
            })
        
        return jsonify({
            'success': True,
            'message': f'ƒê√£ ƒë·ªïi t√™n th∆∞ m·ª•c "{old_folder_name}" th√†nh "{new_folder_name}" th√†nh c√¥ng',
            'data': {
                'old_folder_name': old_folder_name,
                'new_folder_name': new_folder_name,
                'moved_files_count': success_count,
                'moved_files': moved_files
            }
        })
        
    except Exception as e:
        print(f"‚ùå L·ªói rename folder: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'success': False,
            'error': f'L·ªói rename folder: {str(e)}'
        }), 500


# API ƒë·ªÉ x√≥a th∆∞ m·ª•c - FIXED VERSION (S·ª≠ d·ª•ng POST thay v√¨ DELETE ƒë·ªÉ tr√°nh conflict)
@app.route('/api/storage/delete-folder', methods=['POST', 'DELETE'])
def delete_folder():
    """
    API ƒë·ªÉ x√≥a th∆∞ m·ª•c v√† t·∫•t c·∫£ files b√™n trong - Fixed version
    H·ªó tr·ª£ c·∫£ POST v√† DELETE methods
    """
    try:
        if DEMO_MODE:
            return jsonify({
                'success': False,
                'error': 'T√≠nh nƒÉng n√†y kh√¥ng kh·∫£ d·ª•ng ·ªü Demo Mode'
            }), 400
        
        # X·ª≠ l√Ω data t·ª´ request
        if request.method == 'POST':
            data = request.get_json()
        else:  # DELETE method
            data = request.get_json() if request.is_json else {}
            # N·∫øu kh√¥ng c√≥ JSON data, th·ª≠ l·∫•y t·ª´ query params
            if not data:
                folder_name = request.args.get('folder_name', '').strip()
                confirm_delete = request.args.get('confirm_delete', '').lower() == 'true'
                data = {
                    'folder_name': folder_name,
                    'confirm_delete': confirm_delete
                }
        
        if not data:
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng c√≥ d·ªØ li·ªáu request'
            }), 400
            
        folder_name = data.get('folder_name', '').strip()
        confirm_delete = data.get('confirm_delete', False)
        
        if not folder_name:
            return jsonify({
                'success': False,
                'error': 'T√™n th∆∞ m·ª•c kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng'
            }), 400
        
        if not confirm_delete:
            return jsonify({
                'success': False,
                'error': 'Vui l√≤ng x√°c nh·∫≠n x√≥a th∆∞ m·ª•c b·∫±ng c√°ch set confirm_delete = true'
            }), 400
        
        print(f"üóëÔ∏è ƒêang x√≥a folder '{folder_name}'...")
        
        # L·∫•y t·∫•t c·∫£ files trong folder
        all_files = get_all_storage_files(supabase, SUPABASE_BUCKET)
        folder_files = [f for f in all_files if f['folder'] == folder_name]
        
        if not folder_files:
            return jsonify({
                'success': False,
                'error': f'Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c "{folder_name}" ho·∫∑c th∆∞ m·ª•c r·ªóng'
            }), 404
        
        print(f"üìä T√¨m th·∫•y {len(folder_files)} files ƒë·ªÉ x√≥a")
        
        # Chu·∫©n b·ªã danh s√°ch paths ƒë·ªÉ x√≥a
        file_paths = [f['full_path'] for f in folder_files]
        
        # Th·ª±c hi·ªán x√≥a theo batch (Supabase c√≥ th·ªÉ x√≥a nhi·ªÅu files c√πng l√∫c)
        deleted_files = []
        failed_files = []
        
        # Chia nh·ªè th√†nh batches ƒë·ªÉ tr√°nh timeout (20 files/batch ƒë·ªÉ tƒÉng ƒë·ªô ·ªïn ƒë·ªãnh)
        batch_size = 20
        total_batches = (len(file_paths) + batch_size - 1) // batch_size
        
        for i in range(0, len(file_paths), batch_size):
            batch_paths = file_paths[i:i + batch_size]
            batch_files = folder_files[i:i + batch_size]
            current_batch = i // batch_size + 1
            
            try:
                print(f"üóÇÔ∏è X√≥a batch {current_batch}/{total_batches}: {len(batch_paths)} files")
                
                # X√≥a batch files
                delete_response = supabase.storage.from_(SUPABASE_BUCKET).remove(batch_paths)
                
                # Ki·ªÉm tra response (Supabase tr·∫£ v·ªÅ list ho·∫∑c c√≥ th·ªÉ c√≥ error)
                if hasattr(delete_response, 'error') and delete_response.error:
                    # N·∫øu batch fail, th·ª≠ x√≥a t·ª´ng file ri√™ng
                    print(f"‚ö†Ô∏è Batch delete failed: {delete_response.error}, trying individual files...")
                    
                    for j, file_path in enumerate(batch_paths):
                        try:
                            single_response = supabase.storage.from_(SUPABASE_BUCKET).remove([file_path])
                            
                            if hasattr(single_response, 'error') and single_response.error:
                                failed_files.append({
                                    'file': batch_files[j]['name'],
                                    'path': file_path,
                                    'error': str(single_response.error)
                                })
                            else:
                                deleted_files.append({
                                    'file': batch_files[j]['name'],
                                    'path': file_path,
                                    'size': batch_files[j]['size']
                                })
                        except Exception as e:
                            failed_files.append({
                                'file': batch_files[j]['name'],
                                'path': file_path,
                                'error': str(e)
                            })
                else:
                    # Batch delete th√†nh c√¥ng
                    for file_info in batch_files:
                        deleted_files.append({
                            'file': file_info['name'],
                            'path': file_info['full_path'],
                            'size': file_info['size']
                        })
                    
            except Exception as e:
                print(f"‚ùå Error deleting batch {current_batch}: {str(e)}")
                # N·∫øu c√≥ l·ªói v·ªõi batch, th·ª≠ x√≥a t·ª´ng file
                for j, file_path in enumerate(batch_paths):
                    try:
                        single_response = supabase.storage.from_(SUPABASE_BUCKET).remove([file_path])
                        
                        if hasattr(single_response, 'error') and single_response.error:
                            failed_files.append({
                                'file': batch_files[j]['name'],
                                'path': file_path,
                                'error': str(single_response.error)
                            })
                        else:
                            deleted_files.append({
                                'file': batch_files[j]['name'],
                                'path': file_path,
                                'size': batch_files[j]['size']
                            })
                    except Exception as inner_e:
                        failed_files.append({
                            'file': batch_files[j]['name'],
                            'path': file_path,
                            'error': str(inner_e)
                        })
        
        # T√≠nh to√°n k·∫øt qu·∫£
        success_count = len(deleted_files)
        failed_count = len(failed_files)
        total_count = len(folder_files)
        
        if success_count == 0:
            return jsonify({
                'success': False,
                'error': f'Kh√¥ng th·ªÉ x√≥a b·∫•t k·ª≥ file n√†o trong th∆∞ m·ª•c "{folder_name}"',
                'details': {
                    'failed_files': failed_files
                }
            }), 500
        
        # T√≠nh t·ªïng size ƒë√£ x√≥a
        total_deleted_size = sum(f['size'] for f in deleted_files)
        
        print(f"‚úÖ X√≥a folder ho√†n th√†nh: {success_count}/{total_count} files")
        
        # N·∫øu c√≥ m·ªôt s·ªë files fail
        if failed_files:
            return jsonify({
                'success': True,
                'message': f'ƒê√£ x√≥a th∆∞ m·ª•c "{folder_name}" ({success_count}/{total_count} files)',
                'warning': f'{failed_count} files kh√¥ng th·ªÉ x√≥a',
                'data': {
                    'folder_name': folder_name,
                    'deleted_files_count': success_count,
                    'failed_files_count': failed_count,
                    'total_deleted_size': total_deleted_size,
                    'total_deleted_size_human': format_file_size(total_deleted_size),
                    'deleted_files': deleted_files,
                    'failed_files': failed_files
                }
            })
        
        return jsonify({
            'success': True,
            'message': f'ƒê√£ x√≥a th∆∞ m·ª•c "{folder_name}" v√† t·∫•t c·∫£ {success_count} files',
            'data': {
                'folder_name': folder_name,
                'deleted_files_count': success_count,
                'total_deleted_size': total_deleted_size,
                'total_deleted_size_human': format_file_size(total_deleted_size),
                'deleted_files': deleted_files
            }
        })
        
    except Exception as e:
        print(f"‚ùå L·ªói delete folder: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'success': False,
            'error': f'L·ªói delete folder: {str(e)}'
        }), 500


# API ƒë·ªÉ test connection Supabase
@app.route('/api/storage/test-connection', methods=['GET'])
def test_supabase_connection():
    """
    API ƒë·ªÉ test k·∫øt n·ªëi Supabase
    """
    try:
        if DEMO_MODE:
            return jsonify({
                'success': True,
                'message': 'ƒêang ch·∫°y ·ªü Demo Mode',
                'data': {
                    'demo_mode': True,
                    'supabase_configured': False
                }
            })
        
        # Test b·∫±ng c√°ch list files
        test_result = supabase.storage.from_(SUPABASE_BUCKET).list(path="", options={"limit": 1})
        
        return jsonify({
            'success': True,
            'message': 'K·∫øt n·ªëi Supabase th√†nh c√¥ng',
            'data': {
                'demo_mode': False,
                'supabase_configured': True,
                'bucket_name': SUPABASE_BUCKET,
                'supabase_url': SUPABASE_URL,
                'test_result': 'OK'
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'L·ªói k·∫øt n·ªëi Supabase: {str(e)}',
            'data': {
                'demo_mode': DEMO_MODE,
                'supabase_configured': bool(SUPABASE_URL and SUPABASE_KEY),
                'bucket_name': SUPABASE_BUCKET
            }
        }), 500


# API ƒë·ªÉ t·∫°o th∆∞ m·ª•c m·ªõi (Bonus function)
@app.route('/api/storage/create-folder', methods=['POST'])
def create_folder():
    """
    API ƒë·ªÉ t·∫°o th∆∞ m·ª•c m·ªõi b·∫±ng c√°ch upload file placeholder
    """
    try:
        if DEMO_MODE:
            return jsonify({
                'success': False,
                'error': 'T√≠nh nƒÉng n√†y kh√¥ng kh·∫£ d·ª•ng ·ªü Demo Mode'
            }), 400
        
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng c√≥ d·ªØ li·ªáu JSON'
            }), 400
            
        folder_name = data.get('folder_name', '').strip()
        
        if not folder_name:
            return jsonify({
                'success': False,
                'error': 'T√™n th∆∞ m·ª•c kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng'
            }), 400
        
        # Validate t√™n folder
        if '/' in folder_name or '\\' in folder_name:
            return jsonify({
                'success': False,
                'error': 'T√™n th∆∞ m·ª•c kh√¥ng ƒë∆∞·ª£c ch·ª©a k√Ω t·ª± / ho·∫∑c \\'
            }), 400
        
        # Ki·ªÉm tra folder ƒë√£ t·ªìn t·∫°i ch∆∞a
        all_files = get_all_storage_files(supabase, SUPABASE_BUCKET)
        existing_folder = [f for f in all_files if f['folder'] == folder_name]
        
        if existing_folder:
            return jsonify({
                'success': False,
                'error': f'Th∆∞ m·ª•c "{folder_name}" ƒë√£ t·ªìn t·∫°i'
            }), 400
        
        # T·∫°o folder b·∫±ng c√°ch upload file .gitkeep
        placeholder_path = f"{folder_name}/.gitkeep"
        placeholder_content = "# This file keeps the folder structure\n"
        
        upload_response = supabase.storage.from_(SUPABASE_BUCKET).upload(
            path=placeholder_path,
            file=placeholder_content.encode('utf-8'),
            file_options={
                "content-type": "text/plain"
            }
        )
        
        if hasattr(upload_response, 'error') and upload_response.error:
            return jsonify({
                'success': False,
                'error': f'Kh√¥ng th·ªÉ t·∫°o th∆∞ m·ª•c: {upload_response.error}'
            }), 500
        
        return jsonify({
            'success': True,
            'message': f'ƒê√£ t·∫°o th∆∞ m·ª•c "{folder_name}" th√†nh c√¥ng',
            'data': {
                'folder_name': folder_name,
                'placeholder_file': '.gitkeep'
            }
        })
        
    except Exception as e:
        print(f"‚ùå L·ªói create folder: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'L·ªói create folder: {str(e)}'
        }), 500

# API ƒë·ªÉ l·∫•y th√¥ng tin chi ti·∫øt c·ªßa 1 th∆∞ m·ª•c
@app.route('/api/storage/folder-info/<folder_name>', methods=['GET'])
def get_folder_info(folder_name):
    """
    API ƒë·ªÉ l·∫•y th√¥ng tin chi ti·∫øt c·ªßa 1 th∆∞ m·ª•c
    """
    try:
        if DEMO_MODE:
            return jsonify({
                'success': False,
                'error': 'T√≠nh nƒÉng n√†y kh√¥ng kh·∫£ d·ª•ng ·ªü Demo Mode'
            }), 400
        
        folder_name = folder_name.strip()
        if not folder_name:
            return jsonify({
                'success': False,
                'error': 'T√™n th∆∞ m·ª•c kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng'
            }), 400
        
        print(f"üìÅ ƒêang l·∫•y th√¥ng tin folder '{folder_name}'...")
        
        # L·∫•y t·∫•t c·∫£ files trong folder
        all_files = get_all_storage_files(supabase, SUPABASE_BUCKET)
        folder_files = [f for f in all_files if f['folder'] == folder_name]
        
        if not folder_files:
            return jsonify({
                'success': False,
                'error': f'Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c "{folder_name}"'
            }), 404
        
        # T√≠nh to√°n th·ªëng k√™
        total_size = sum(f['size'] for f in folder_files)
        file_types = {}
        
        for file_info in folder_files:
            file_name = file_info['name']
            file_ext = file_name.split('.')[-1].lower() if '.' in file_name else 'no_extension'
            
            if file_ext not in file_types:
                file_types[file_ext] = {
                    'count': 0,
                    'total_size': 0
                }
            
            file_types[file_ext]['count'] += 1
            file_types[file_ext]['total_size'] += file_info['size']
        
        # Format file types
        for ext, info in file_types.items():
            info['total_size_human'] = format_file_size(info['total_size'])
        
        # S·∫Øp x·∫øp files theo size (l·ªõn nh·∫•t tr∆∞·ªõc)
        sorted_files = sorted(folder_files, key=lambda x: x['size'], reverse=True)
        
        return jsonify({
            'success': True,
            'data': {
                'folder_name': folder_name,
                'file_count': len(folder_files),
                'total_size': total_size,
                'total_size_human': format_file_size(total_size),
                'file_types': file_types,
                'files': sorted_files,
                'largest_files': sorted_files[:5],  # Top 5 files l·ªõn nh·∫•t
                'statistics': {
                    'total_file_types': len(file_types),
                    'average_file_size': total_size // len(folder_files) if folder_files else 0,
                    'average_file_size_human': format_file_size(total_size // len(folder_files)) if folder_files else '0 B'
                }
            }
        })
        
    except Exception as e:
        print(f"‚ùå L·ªói l·∫•y th√¥ng tin folder: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'L·ªói l·∫•y th√¥ng tin folder: {str(e)}'
        }), 500
      
@app.route('/api/submissions', methods=['GET'])
def get_submissions():
    try:
        # L·∫•y query parameters
        page = int(request.args.get('page', 1))
        limit = min(int(request.args.get('limit', 10)), 100)  # Max 100 records per page
        sort_by = request.args.get('sort_by', 'upload_time')
        sort_order = request.args.get('sort_order', 'desc').lower()
        folder_filter = request.args.get('folder', '').strip()
        search_query = request.args.get('search', '').strip()
        date_from = request.args.get('date_from', '').strip()
        date_to = request.args.get('date_to', '').strip()
        has_file_filter = request.args.get('has_file', '').strip().lower()
        
        # Validate parameters
        if page < 1:
            page = 1
        if limit < 1:
            limit = 10
        if sort_order not in ['asc', 'desc']:
            sort_order = 'desc'
        
        # Allowed sort fields
        allowed_sort_fields = ['upload_time', 'ho_ten', 'ten_de_tai', 'gio_quy_doi', 'folder_name', 'file_size']
        if sort_by not in allowed_sort_fields:
            sort_by = 'upload_time'
        
        # Handle demo mode
        if DEMO_MODE:
            # Generate demo data
            demo_submissions = []
            for i in range(1, 25):  # 24 demo records
                demo_submissions.append({
                    'id': i,
                    'ho_ten': f'Demo User {i}',
                    'ten_de_tai': f'Demo Project {i}',
                    'noi_cong_tac': f'Demo Company {i}',
                    'khoa_phong': f'Demo Department {i}',
                    'gio_quy_doi': round(i * 1.5, 2),
                    'minh_chung': f'Demo evidence {i}',
                    'ghi_chu': f'Demo note {i}',
                    'file_name': f'demo_file_{i}.pdf' if i % 2 == 0 else None,
                    'file_url': f'https://demo.example.com/demo_file_{i}.pdf' if i % 2 == 0 else None,
                    'file_size': i * 1024 if i % 2 == 0 else 0,
                    'folder_name': f'Demo Folder {(i % 5) + 1}' if i % 3 == 0 else None,
                    'upload_time': (datetime.datetime.now() - datetime.timedelta(days=i)).isoformat(),
                    'upload_ip': '127.0.0.1',
                    'storage_path': f'demo-folder-{(i % 5) + 1}/demo_file_{i}.pdf' if i % 2 == 0 else None
                })
            
            # Apply filters to demo data
            filtered_submissions = demo_submissions
            
            # Apply search filter
            if search_query:
                filtered_submissions = [
                    s for s in filtered_submissions 
                    if search_query.lower() in s['ho_ten'].lower() or 
                       search_query.lower() in s['ten_de_tai'].lower()
                ]
            
            # Apply folder filter
            if folder_filter:
                filtered_submissions = [
                    s for s in filtered_submissions 
                    if s['folder_name'] and folder_filter.lower() in s['folder_name'].lower()
                ]
            
            # Apply has_file filter
            if has_file_filter == 'true':
                filtered_submissions = [s for s in filtered_submissions if s['file_name']]
            elif has_file_filter == 'false':
                filtered_submissions = [s for s in filtered_submissions if not s['file_name']]
            
            # Apply sorting
            reverse_sort = sort_order == 'desc'
            filtered_submissions.sort(key=lambda x: x.get(sort_by, ''), reverse=reverse_sort)
            
            # Apply pagination
            offset = (page - 1) * limit
            paginated_submissions = filtered_submissions[offset:offset + limit]
            
            return jsonify({
                'success': True,
                'data': paginated_submissions,
                'pagination': {
                    'current_page': page,
                    'per_page': limit,
                    'total_records': len(filtered_submissions),
                    'total_pages': math.ceil(len(filtered_submissions) / limit),
                    'has_next': page < math.ceil(len(filtered_submissions) / limit),
                    'has_prev': page > 1
                },
                'filters': {
                    'folder': folder_filter,
                    'search': search_query,
                    'date_from': date_from,
                    'date_to': date_to,
                    'has_file': has_file_filter,
                    'sort_by': sort_by,
                    'sort_order': sort_order
                },
                'demo_mode': True
            })
        
        # Real Supabase query
        query = supabase.table('submissions').select('*')
        
        # Apply filters
        if folder_filter:
            query = query.ilike('folder_name', f'%{folder_filter}%')
        
        if search_query:
            # Search in multiple fields
            query = query.or_(f'ho_ten.ilike.%{search_query}%,ten_de_tai.ilike.%{search_query}%')
        
        if date_from:
            try:
                # Convert date format
                date_from_formatted = datetime.datetime.strptime(date_from, '%Y-%m-%d').isoformat()
                query = query.gte('upload_time', date_from_formatted)
            except ValueError:
                pass  # Invalid date format, ignore
        
        if date_to:
            try:
                # Convert date format and add end of day
                date_to_formatted = datetime.datetime.strptime(date_to, '%Y-%m-%d')
                date_to_formatted = date_to_formatted.replace(hour=23, minute=59, second=59).isoformat()
                query = query.lte('upload_time', date_to_formatted)
            except ValueError:
                pass  # Invalid date format, ignore
        
        if has_file_filter == 'true':
            query = query.not_.is_('file_name', 'null')
        elif has_file_filter == 'false':
            query = query.is_('file_name', 'null')
        
        # Get total count (before pagination)
        count_result = query.execute()
        total_records = len(count_result.data)
        
        # Apply sorting and pagination
        query = query.order(sort_by, desc=(sort_order == 'desc'))
        
        # Apply pagination
        offset = (page - 1) * limit
        query = query.limit(limit).offset(offset)
        
        # Execute query
        result = query.execute()
        
        # Process results
        submissions = []
        for submission in result.data:
            # Format file size
            file_size = submission.get('file_size', 0)
            if file_size:
                file_size_human = format_file_size(file_size)
            else:
                file_size_human = '0 B'
            
            # Format upload time
            upload_time = submission.get('upload_time')
            if upload_time:
                try:
                    upload_datetime = datetime.datetime.fromisoformat(upload_time.replace('Z', '+00:00'))
                    upload_time_formatted = upload_datetime.strftime('%d/%m/%Y %H:%M')
                except:
                    upload_time_formatted = upload_time
            else:
                upload_time_formatted = 'N/A'
            
            processed_submission = {
                **submission,
                'file_size_human': file_size_human,
                'upload_time_formatted': upload_time_formatted,
                'has_file': bool(submission.get('file_name')),
                'folder_display': submission.get('folder_name') or 'Kh√¥ng c√≥ th∆∞ m·ª•c'
            }
            
            submissions.append(processed_submission)
        
        # Calculate pagination info
        total_pages = math.ceil(total_records / limit)
        
        return jsonify({
            'success': True,
            'data': submissions,
            'pagination': {
                'current_page': page,
                'per_page': limit,
                'total_records': total_records,
                'total_pages': total_pages,
                'has_next': page < total_pages,
                'has_prev': page > 1,
                'offset': offset
            },
            'filters': {
                'folder': folder_filter,
                'search': search_query,
                'date_from': date_from,
                'date_to': date_to,
                'has_file': has_file_filter,
                'sort_by': sort_by,
                'sort_order': sort_order
            },
            'demo_mode': False
        })
        
    except Exception as e:
        print(f"Error in get_submissions: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'success': False,
            'error': f'L·ªói l·∫•y danh s√°ch submissions: {str(e)}'
        }), 500

@app.route('/api/submissions/<int:submission_id>', methods=['GET'])
def get_submission_detail(submission_id):
    """API ƒë·ªÉ l·∫•y chi ti·∫øt m·ªôt submission"""
    try:
        if DEMO_MODE:
            # Return demo data
            return jsonify({
                'success': True,
                'data': {
                    'id': submission_id,
                    'ho_ten': f'Demo User {submission_id}',
                    'ten_de_tai': f'Demo Project {submission_id}',
                    'noi_cong_tac': f'Demo Company {submission_id}',
                    'khoa_phong': f'Demo Department {submission_id}',
                    'gio_quy_doi': submission_id * 1.5,
                    'minh_chung': f'Demo evidence {submission_id}',
                    'ghi_chu': f'Demo note {submission_id}',
                    'file_name': f'demo_file_{submission_id}.pdf',
                    'file_url': f'https://demo.example.com/demo_file_{submission_id}.pdf',
                    'file_size': submission_id * 1024,
                    'file_size_human': format_file_size(submission_id * 1024),
                    'folder_name': f'Demo Folder {submission_id}',
                    'upload_time': datetime.datetime.now().isoformat(),
                    'upload_ip': '127.0.0.1',
                    'storage_path': f'demo-folder-{submission_id}/demo_file_{submission_id}.pdf'
                },
                'demo_mode': True
            })
        
        # Real query
        result = supabase.table('submissions').select('*').eq('id', submission_id).single().execute()
        
        if not result.data:
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng t√¨m th·∫•y submission'
            }), 404
        
        submission = result.data
        
        # Add formatted fields
        submission['file_size_human'] = format_file_size(submission.get('file_size', 0))
        submission['has_file'] = bool(submission.get('file_name'))
        submission['folder_display'] = submission.get('folder_name') or 'Kh√¥ng c√≥ th∆∞ m·ª•c'
        
        # Format upload time
        upload_time = submission.get('upload_time')
        if upload_time:
            try:
                upload_datetime = datetime.datetime.fromisoformat(upload_time.replace('Z', '+00:00'))
                submission['upload_time_formatted'] = upload_datetime.strftime('%d/%m/%Y %H:%M:%S')
            except:
                submission['upload_time_formatted'] = upload_time
        
        return jsonify({
            'success': True,
            'data': submission
        })
        
    except Exception as e:
        print(f"Error in get_submission_detail: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'L·ªói l·∫•y chi ti·∫øt submission: {str(e)}'
        }), 500

@app.route('/api/submissions/<int:submission_id>', methods=['PUT'])
def update_submission(submission_id):
    """API ƒë·ªÉ c·∫≠p nh·∫≠t m·ªôt submission"""
    try:
        if DEMO_MODE:
            return jsonify({
                'success': True,
                'message': 'Demo mode: C·∫≠p nh·∫≠t th√†nh c√¥ng (gi·∫£ l·∫≠p)',
                'demo_mode': True
            })
        
        # Get request data
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ c·∫≠p nh·∫≠t'
            }), 400
        
        # Allowed fields to update
        allowed_fields = [
            'ho_ten', 'ten_de_tai', 'noi_cong_tac', 'khoa_phong', 
            'gio_quy_doi', 'minh_chung', 'ghi_chu', 'folder_name'
        ]
        
        # Filter and validate data
        update_data = {}
        for field in allowed_fields:
            if field in data:
                value = data[field]
                if field == 'gio_quy_doi':
                    try:
                        value = float(value)
                    except (ValueError, TypeError):
                        value = 0.0
                elif field == 'folder_name' and value:
                    # Validate folder name
                    safe_folder_name = secure_folder_name(value)
                    if not safe_folder_name:
                        return jsonify({
                            'success': False,
                            'error': 'T√™n th∆∞ m·ª•c kh√¥ng h·ª£p l·ªá'
                        }), 400
                    value = safe_folder_name
                
                update_data[field] = value
        
        if not update_data:
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá ƒë·ªÉ c·∫≠p nh·∫≠t'
            }), 400
        
        # Update in database
        result = supabase.table('submissions').update(update_data).eq('id', submission_id).execute()
        
        if not result.data:
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng t√¨m th·∫•y ho·∫∑c kh√¥ng th·ªÉ c·∫≠p nh·∫≠t submission'
            }), 404
        
        return jsonify({
            'success': True,
            'message': 'C·∫≠p nh·∫≠t submission th√†nh c√¥ng',
            'data': result.data[0]
        })
        
    except Exception as e:
        print(f"Error in update_submission: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'L·ªói c·∫≠p nh·∫≠t submission: {str(e)}'
        }), 500

@app.route('/api/submissions/<int:submission_id>', methods=['DELETE'])
def delete_submission(submission_id):
    """API ƒë·ªÉ x√≥a m·ªôt submission"""
    try:
        if DEMO_MODE:
            return jsonify({
                'success': True,
                'message': 'Demo mode: X√≥a th√†nh c√¥ng (gi·∫£ l·∫≠p)',
                'demo_mode': True
            })
        
        # Get submission info first
        submission_result = supabase.table('submissions').select('*').eq('id', submission_id).single().execute()
        
        if not submission_result.data:
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng t√¨m th·∫•y submission'
            }), 404
        
        submission = submission_result.data
        storage_path = submission.get('storage_path')
        
        # Delete file from storage if exists
        if storage_path:
            try:
                delete_result = supabase.storage.from_(SUPABASE_BUCKET).remove([storage_path])
                print(f"File deleted from storage: {storage_path}")
            except Exception as storage_error:
                print(f"Error deleting file from storage: {str(storage_error)}")
                # Continue with database deletion even if file deletion fails
        
        # Delete from database
        delete_result = supabase.table('submissions').delete().eq('id', submission_id).execute()
        
        if not delete_result.data:
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng th·ªÉ x√≥a submission'
            }), 500
        
        return jsonify({
            'success': True,
            'message': 'X√≥a submission th√†nh c√¥ng',
            'deleted_file': storage_path
        })
        
    except Exception as e:
        print(f"Error in delete_submission: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'L·ªói x√≥a submission: {str(e)}'
        }), 500

@app.route('/api/submissions/stats', methods=['GET'])
def get_submissions_stats():
    """API ƒë·ªÉ l·∫•y th·ªëng k√™ submissions"""
    try:
        if DEMO_MODE:
            return jsonify({
                'success': True,
                'data': {
                    'total_submissions': 24,
                    'submissions_with_files': 12,
                    'submissions_without_files': 12,
                    'total_file_size': 24576,
                    'total_file_size_human': format_file_size(24576),
                    'total_folders': 5,
                    'avg_file_size': 2048,
                    'avg_file_size_human': format_file_size(2048),
                    'submissions_by_folder': {
                        'Demo Folder 1': 5,
                        'Demo Folder 2': 4,
                        'Demo Folder 3': 3,
                        'Kh√¥ng c√≥ th∆∞ m·ª•c': 12
                    },
                    'submissions_by_month': {
                        '2024-01': 8,
                        '2024-02': 10,
                        '2024-03': 6
                    },
                    'file_types': {
                        'pdf': 8,
                        'docx': 3,
                        'xlsx': 1
                    }
                },
                'demo_mode': True
            })
        
        # Real statistics
        result = supabase.table('submissions').select('*').execute()
        submissions = result.data
        
        total_submissions = len(submissions)
        submissions_with_files = len([s for s in submissions if s.get('file_name')])
        submissions_without_files = total_submissions - submissions_with_files
        
        # File size statistics
        file_sizes = [s.get('file_size', 0) for s in submissions if s.get('file_size')]
        total_file_size = sum(file_sizes)
        avg_file_size = total_file_size / len(file_sizes) if file_sizes else 0
        
        # Folder statistics
        folder_counts = {}
        for submission in submissions:
            folder = submission.get('folder_name') or 'Kh√¥ng c√≥ th∆∞ m·ª•c'
            folder_counts[folder] = folder_counts.get(folder, 0) + 1
        
        # Monthly statistics
        monthly_counts = {}
        for submission in submissions:
            upload_time = submission.get('upload_time')
            if upload_time:
                try:
                    date = datetime.datetime.fromisoformat(upload_time.replace('Z', '+00:00'))
                    month_key = date.strftime('%Y-%m')
                    monthly_counts[month_key] = monthly_counts.get(month_key, 0) + 1
                except:
                    pass
        
        # File type statistics
        file_type_counts = {}
        for submission in submissions:
            file_name = submission.get('file_name')
            if file_name and '.' in file_name:
                ext = file_name.split('.')[-1].lower()
                file_type_counts[ext] = file_type_counts.get(ext, 0) + 1
        
        return jsonify({
            'success': True,
            'data': {
                'total_submissions': total_submissions,
                'submissions_with_files': submissions_with_files,
                'submissions_without_files': submissions_without_files,
                'total_file_size': total_file_size,
                'total_file_size_human': format_file_size(total_file_size),
                'total_folders': len([f for f in folder_counts.keys() if f != 'Kh√¥ng c√≥ th∆∞ m·ª•c']),
                'avg_file_size': int(avg_file_size),
                'avg_file_size_human': format_file_size(avg_file_size),
                'submissions_by_folder': folder_counts,
                'submissions_by_month': monthly_counts,
                'file_types': file_type_counts
            }
        })
        
    except Exception as e:
        print(f"Error in get_submissions_stats: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'L·ªói l·∫•y th·ªëng k√™ submissions: {str(e)}'
        }), 500

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=int(os.environ.get('PORT', 5000)))