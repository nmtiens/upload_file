# Th√™m c√°c import c·∫ßn thi·∫øt ·ªü ƒë·∫ßu file
import hashlib
import json
import logging
import tempfile
import re  # Th√™m import re cho h√†m secure_folder_name
from typing import Optional
from flask import Flask, request, render_template, jsonify, send_file, abort
import os
from urllib.parse import unquote
from datetime import datetime
# S·ª≠ d·ª•ng: datetime.now()
import pytz
from werkzeug.utils import secure_filename
import math
from pathlib import Path
import platform
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import os

import uuid
from supabase import create_client, Client  
import zipfile
import shutil
from pathlib import Path

from utils import format_file_size, secure_folder_name
# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
# C·∫•u h√¨nh Supabase v·ªõi gi√° tr·ªã m·∫∑c ƒë·ªãnh v√† ki·ªÉm tra t·ªët h∆°n
SUPABASE_URL = os.environ.get('SUPABASE_URL', '')
SUPABASE_KEY = os.environ.get('SUPABASE_KEY', '')
SUPABASE_BUCKET = os.environ.get('SUPABASE_BUCKET', 'file-uploads')


# Ki·ªÉm tra v√† h∆∞·ªõng d·∫´n c·∫•u h√¨nh
if not SUPABASE_URL or not SUPABASE_KEY:
    print("=" * 60)
    print("‚ùå C·∫¢NH B√ÅO: Thi·∫øu c·∫•u h√¨nh Supabase!")
    print("=" * 60)
    print("Vui l√≤ng c·∫•u h√¨nh c√°c bi·∫øn m√¥i tr∆∞·ªùng sau:")
    print("")
    print("üîß C√ÅCH 1: S·ª≠ d·ª•ng file .env")
    print("T·∫°o file .env trong th∆∞ m·ª•c g·ªëc v·ªõi n·ªôi dung:")
    print("SUPABASE_URL=https://your-project-ref.supabase.co")
    print("SUPABASE_KEY=your-anon-public-key")
    print("SUPABASE_BUCKET=file-uploads")
    print("")
    print("üîß C√ÅCH 2: Set bi·∫øn m√¥i tr∆∞·ªùng (Windows)")
    print("set SUPABASE_URL=https://your-project-ref.supabase.co")
    print("set SUPABASE_KEY=your-anon-public-key")
    print("set SUPABASE_BUCKET=file-uploads")
    print("")
    print("üîß C√ÅCH 3: Set bi·∫øn m√¥i tr∆∞·ªùng (Linux/Mac)")
    print("export SUPABASE_URL=https://your-project-ref.supabase.co")
    print("export SUPABASE_KEY=your-anon-public-key")
    print("export SUPABASE_BUCKET=file-uploads")
    print("")
    print("üìã L·∫•y th√¥ng tin Supabase:")
    print("1. ƒêƒÉng nh·∫≠p v√†o https://supabase.com")
    print("2. Ch·ªçn project c·ªßa b·∫°n")
    print("3. V√†o Settings > API")
    print("4. Copy URL v√† anon/public key")
    print("")
    print("=" * 60)
    
    # Cho ph√©p ch·∫°y ·ªü ch·∫ø ƒë·ªô demo (kh√¥ng k·∫øt n·ªëi Supabase)
    print("üöÄ Kh·ªüi ƒë·ªông ·ªü ch·∫ø ƒë·ªô DEMO (kh√¥ng c√≥ Supabase)")
    print("·ª®ng d·ª•ng s·∫Ω ch·∫°y nh∆∞ng kh√¥ng th·ªÉ upload file th·∫≠t")
    print("=" * 60)
    
    # T·∫°o mock supabase client ƒë·ªÉ tr√°nh l·ªói
    class MockSupabase:
        def __init__(self):
            self.storage_instance = MockStorage()
            self.table_instance = MockTable()
        
        def storage(self):
            return self.storage_instance
        
        def table(self, table_name):
            return self.table_instance
    
    class MockStorage:
        def from_(self, bucket):
            return MockBucket()
    
    class MockBucket:
        def upload(self, path, file, file_options=None):
            return MockResult(200)
        
        def get_public_url(self, path):
            return f"https://demo.example.com/{path}"
        
        def list(self, path=""):
            return []
        
        def download(self, path):
            return b"Demo file content"
    
    class MockTable:
        def insert(self, data):
            return MockResult(200, data)
        
        def select(self, fields="*"):
            return self
        
        def eq(self, field, value):
            return self
        
        def single(self):
            return self
        
        def limit(self, count):
            return self
        
        def order(self, field, desc=False):
            return self
        
        def execute(self):
            return MockResult(200, [])
    
    class MockResult:
        def __init__(self, status_code, data=None):
            self.status_code = status_code
            self.data = data or []
    
    supabase = MockSupabase()
    DEMO_MODE = True
else:
    # Kh·ªüi t·∫°o Supabase client th·∫≠t
    try:
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        DEMO_MODE = False
        print(f"‚úÖ K·∫øt n·ªëi Supabase th√†nh c√¥ng!")
        print(f"üìç URL: {SUPABASE_URL}")
        print(f"üóÇÔ∏è Bucket: {SUPABASE_BUCKET}")
    except Exception as e:
        print(f"‚ùå L·ªói k·∫øt n·ªëi Supabase: {str(e)}")
        print("üîÑ Chuy·ªÉn sang ch·∫ø ƒë·ªô DEMO")
        # S·ª≠ d·ª•ng mock client
        DEMO_MODE = True
# Cache ƒë·ªÉ tr√°nh g·ªçi API nhi·ªÅu l·∫ßn



# C·∫•u h√¨nh app
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size

# In th√¥ng tin kh·ªüi ƒë·ªông
print(f"=== FILE UPLOAD SERVER ===")
if DEMO_MODE:
    print("‚ö†Ô∏è  CH·∫†Y ·ªû CH·∫æ ƒê·ªò DEMO")
    print("üìù Ch·ª©c nƒÉng upload s·∫Ω m√¥ ph·ªèng")
else:
    print("‚úÖ CH·∫†Y V·ªöI SUPABASE")
    print(f"üåê Supabase URL: {SUPABASE_URL}")
    print(f"üóÇÔ∏è Bucket: {SUPABASE_BUCKET}")
print("=" * 30)

def init_db():
    """Kh·ªüi t·∫°o b·∫£ng submissions trong Supabase"""
    if DEMO_MODE:
        print("üìù Demo mode: B·ªè qua ki·ªÉm tra database")
        return
    
    try:
        # T·∫°o b·∫£ng submissions n·∫øu ch∆∞a c√≥
        result = supabase.table('submissions').select('*').limit(1).execute()
        print("‚úÖ Database connection successful!")
    except Exception as e:
        print(f"‚ùå Database initialization error: {str(e)}")
        print("üìã Vui l√≤ng t·∫°o b·∫£ng 'submissions' trong Supabase v·ªõi schema sau:")
        print("""
        CREATE TABLE submissions (
            id SERIAL PRIMARY KEY,
            ho_ten TEXT NOT NULL,
            noi_cong_tac TEXT,
            khoa_phong TEXT,
            ten_de_tai TEXT NOT NULL,
            gio_quy_doi REAL DEFAULT 0,
            minh_chung TEXT,
            ghi_chu TEXT,
            file_name TEXT,
            file_url TEXT,
            file_size INTEGER DEFAULT 0,
            folder_name TEXT,
            upload_time TIMESTAMP DEFAULT NOW(),
            upload_ip TEXT,
            storage_path TEXT
        );
        """)

init_db()

ALLOWED_EXTENSIONS = {'txt', 'pdf', 'png', 'jpg', 'jpeg', 'gif', 'doc', 'docx', 'xls', 'xlsx', 'zip', 'rar', '7z'}

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def secure_filename_vietnamese(filename):
    """
    Chuy·ªÉn ƒë·ªïi t√™n file th√†nh format an to√†n, h·ªó tr·ª£ ti·∫øng Vi·ªát
    """
    if not filename:
        return filename
    
    # T√°ch t√™n file v√† extension
    name, ext = os.path.splitext(filename)
    
    # S·ª≠ d·ª•ng h√†m secure_folder_name ƒë·ªÉ x·ª≠ l√Ω ph·∫ßn t√™n
    safe_name = secure_folder_name(name)
    
    if not safe_name:
        # Fallback n·∫øu kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c
        safe_name = secure_filename(name)
    
    return f"{safe_name}{ext}"

def get_client_ip():
    """L·∫•y IP client"""
    if request.environ.get('HTTP_X_FORWARDED_FOR') is None:
        return request.environ['REMOTE_ADDR']
    else:
        return request.environ['HTTP_X_FORWARDED_FOR']

def calculate_file_checksum(file_path: Path) -> Optional[str]:
    """T√≠nh MD5 checksum c·ªßa file"""
    try:
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception:
        return None




def upload_to_supabase(file, folder_name=None):
    """Upload file l√™n Supabase Storage - Enhanced version with better error handling"""
    
    if DEMO_MODE:
        # M√¥ ph·ªèng upload th√†nh c√¥ng
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        unique_id = str(uuid.uuid4())[:8]
        original_name = secure_filename_vietnamese(file.filename)
        name, ext = os.path.splitext(original_name)
        file_name = f"{name}_{timestamp}_{unique_id}{ext}"
        
        if folder_name:
            storage_path = f"{folder_name}/{file_name}"
        else:
            storage_path = file_name
        
        return {
            'success': True,
            'file_name': file_name,
            'storage_path': storage_path,
            'file_url': f"https://demo.example.com/{storage_path}",
            'file_size': 1024
        }
    
    try:
        # T·∫°o t√™n file unique
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        unique_id = str(uuid.uuid4())[:8]
        original_name = secure_filename_vietnamese(file.filename)
        name, ext = os.path.splitext(original_name)
        file_name = f"{name}_{timestamp}_{unique_id}{ext}"
        
        # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n trong bucket
        if folder_name:
            storage_path = f"{folder_name}/{file_name}"
        else:
            storage_path = file_name
        
        # ƒê·ªçc file content
        file.seek(0)  # Reset file pointer
        file_content = file.read()
        file_size = len(file_content)
        
        if file_size == 0:
            return {
                'success': False,
                'error': "File is empty"
            }
        
        # Ki·ªÉm tra k·∫øt n·ªëi Supabase
        if not supabase:
            return {
                'success': False,
                'error': "Supabase client not initialized"
            }
        
        # Upload l√™n Supabase Storage - Enhanced version
        try:
            # Reset file pointer again before upload
            file.seek(0)
            
            # Attempt upload with different approaches
            storage_client = supabase.storage.from_(SUPABASE_BUCKET)
            
            # Method 1: Direct upload with file object
            try:
                result = storage_client.upload(
                    path=storage_path,
                    file=file_content,
                    file_options={
                        "content-type": file.content_type or "application/octet-stream",
                        "upsert": "false"  # Prevent overwrite
                    }
                )
                
                # Debug: Log the result structure
                logging.debug(f"Upload result type: {type(result)}")
                logging.debug(f"Upload result: {result}")
                
                # Enhanced response parsing
                success = False
                error_msg = None
                
                # Case 1: New supabase-py version (dict response)
                if isinstance(result, dict):
                    if result.get('error'):
                        error_msg = str(result['error'])
                    elif result.get('data') or 'path' in result:
                        success = True
                    else:
                        error_msg = "Unknown dict response format"
                
                # Case 2: Object with attributes
                elif hasattr(result, 'data') and hasattr(result, 'error'):
                    if result.error:
                        error_msg = str(result.error)
                    elif result.data:
                        success = True
                    else:
                        error_msg = "No data in response"
                
                # Case 3: Direct success (some versions return path directly)
                elif isinstance(result, str):
                    success = True
                
                # Case 4: Other object types
                else:
                    # Try to convert to dict
                    try:
                        result_dict = result.__dict__ if hasattr(result, '__dict__') else {}
                        if result_dict.get('data') or result_dict.get('path'):
                            success = True
                        else:
                            error_msg = f"Unknown response object: {type(result)}"
                    except:
                        error_msg = f"Cannot parse response type: {type(result)}"
                
                if success:
                    # Get public URL
                    try:
                        public_url_result = storage_client.get_public_url(storage_path)
                        
                        # Handle different URL response formats
                        if isinstance(public_url_result, str):
                            file_url = public_url_result
                        elif isinstance(public_url_result, dict):
                            file_url = public_url_result.get('publicUrl') or public_url_result.get('url')
                        elif hasattr(public_url_result, 'publicUrl'):
                            file_url = public_url_result.publicUrl
                        else:
                            file_url = f"{SUPABASE_URL}/storage/v1/object/public/{SUPABASE_BUCKET}/{storage_path}"
                        
                        return {
                            'success': True,
                            'file_name': file_name,
                            'storage_path': storage_path,
                            'file_url': file_url,
                            'file_size': file_size
                        }
                    except Exception as url_error:
                        # Upload succeeded but URL generation failed
                        fallback_url = f"{SUPABASE_URL}/storage/v1/object/public/{SUPABASE_BUCKET}/{storage_path}"
                        return {
                            'success': True,
                            'file_name': file_name,
                            'storage_path': storage_path,
                            'file_url': fallback_url,
                            'file_size': file_size,
                            'warning': f"URL generation warning: {str(url_error)}"
                        }
                else:
                    return {
                        'success': False,
                        'error': f"Upload failed: {error_msg or 'Unknown error'}"
                    }
                    
            except Exception as upload_error:
                error_str = str(upload_error)
                
                # Handle specific Supabase errors
                if "already exists" in error_str.lower():
                    return {
                        'success': False,
                        'error': "File already exists. Please try again."
                    }
                elif "permission" in error_str.lower():
                    return {
                        'success': False,
                        'error': "Permission denied. Check bucket policies."
                    }
                elif "size" in error_str.lower():
                    return {
                        'success': False,
                        'error': f"File size error: {error_str}"
                    }
                else:
                    return {
                        'success': False,
                        'error': f"Upload exception: {error_str}"
                    }
                    
        except Exception as storage_error:
            return {
                'success': False,
                'error': f"Storage client error: {str(storage_error)}"
            }
            
    except Exception as e:
        return {
            'success': False,
            'error': f"General error: {str(e)}"
        }

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/info')
def server_info():
    """API ƒë·ªÉ l·∫•y th√¥ng tin server"""
    storage_info = "Demo Mode (No Supabase)" if DEMO_MODE else "Supabase Storage"
    
    return jsonify({
        'storage_type': storage_info,
        'demo_mode': DEMO_MODE,
        'supabase_url': SUPABASE_URL if not DEMO_MODE else "Not configured",
        'supabase_bucket': SUPABASE_BUCKET,
        'server_platform': platform.system(),
        'allowed_extensions': list(ALLOWED_EXTENSIONS),
        'max_file_size': app.config['MAX_CONTENT_LENGTH'],
        'max_file_size_human': format_file_size(app.config['MAX_CONTENT_LENGTH'])
    })

@app.route('/upload', methods=['POST'])
def upload_file():
    try:
        # L·∫•y IP client
        client_ip = get_client_ip()
        
        # L·∫•y d·ªØ li·ªáu form
        ho_ten = request.form.get('ho_ten', '').strip()
        ten_de_tai = request.form.get('ten_de_tai', '').strip()
        noi_cong_tac = request.form.get('noi_cong_tac', '').strip()
        khoa_phong = request.form.get('khoa_phong', '').strip()
        gio_quy_doi = request.form.get('gio_quy_doi', '0')
        minh_chung = request.form.get('minh_chung', '').strip()
        ghi_chu = request.form.get('ghi_chu', '').strip()
        folder_name = request.form.get('folder', '').strip()

        # Ch·ªâ ki·ªÉm tra h·ªç t√™n v√† t√™n ƒë·ªÅ t√†i
        if not all([ho_ten, ten_de_tai]):
            return jsonify({'error': 'Vui l√≤ng ƒëi·ªÅn ƒë·∫ßy ƒë·ªß h·ªç t√™n v√† t√™n ƒë·ªÅ t√†i'}), 400

        try:
            gio_quy_doi = float(gio_quy_doi)
        except ValueError:
            gio_quy_doi = 0.0

        file_url = None
        file_name = None
        file_size = 0
        storage_path = None
        final_folder_name = None

        # X·ª≠ l√Ω folder name
        if folder_name:
            safe_folder_name = secure_folder_name(folder_name)
            if not safe_folder_name:
                return jsonify({'error': 'T√™n th∆∞ m·ª•c kh√¥ng h·ª£p l·ªá'}), 400
            final_folder_name = safe_folder_name

        # X·ª≠ l√Ω file upload
        if 'file' in request.files:
            file = request.files['file']
            if file and hasattr(file, 'filename') and file.filename and file.filename.strip():
                if allowed_file(file.filename):
                    # Upload l√™n Supabase (ho·∫∑c m√¥ ph·ªèng)
                    upload_result = upload_to_supabase(file, final_folder_name)
                    
                    if upload_result['success']:
                        file_name = upload_result['file_name']
                        file_url = upload_result['file_url']
                        file_size = upload_result['file_size']
                        storage_path = upload_result['storage_path']
                        
                        status_text = "Demo upload" if DEMO_MODE else "Uploaded to Supabase"
                        print(f"{status_text}: {storage_path} ({format_file_size(file_size)}) from IP: {client_ip}")
                    else:
                        return jsonify({'error': f'L·ªói upload: {upload_result["error"]}'}), 500
                else:
                    return jsonify({'error': 'Lo·∫°i file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£'}), 400

        # L∆∞u v√†o database (ho·∫∑c m√¥ ph·ªèng)
        try:
            submission_data = {
                'ho_ten': ho_ten,
                'ten_de_tai': ten_de_tai,
                'noi_cong_tac': noi_cong_tac,
                'khoa_phong': khoa_phong,
                'gio_quy_doi': gio_quy_doi,
                'minh_chung': minh_chung,
                'ghi_chu': ghi_chu,
                'file_name': file_name,
                'file_url': file_url,
                'file_size': file_size,
                'folder_name': final_folder_name,
                'upload_time': datetime.now().isoformat(),
                'upload_ip': client_ip,
                'storage_path': storage_path
            }
            
            if not DEMO_MODE:
                result = supabase.table('submissions').insert(submission_data).execute()
                print(f"Data saved to Supabase database: {result}")
            else:
                print(f"Demo mode - would save: {submission_data}")
            
        except Exception as e:
            print(f"Database error: {str(e)}")
            if not DEMO_MODE:
                return jsonify({'error': f'L·ªói l∆∞u database: {str(e)}'}), 500

        # T·∫°o message ph·∫£n h·ªìi
        if DEMO_MODE:
            message = "Demo upload th√†nh c√¥ng! (Kh√¥ng c√≥ Supabase th·∫≠t)"
        else:
            if final_folder_name:
                message = f'ƒê√£ upload th√†nh c√¥ng v√†o th∆∞ m·ª•c "{final_folder_name}" tr√™n Supabase'
            else:
                message = 'Upload th√†nh c√¥ng l√™n Supabase Storage!'
        
        if file_url:
            message += f' - URL: {file_url}'

        return jsonify({
            'message': message,
            'file_name': file_name,
            'file_url': file_url,
            'file_size': file_size,
            'file_size_human': format_file_size(file_size),
            'folder': final_folder_name,
            'storage_path': storage_path,
            'client_ip': client_ip,
            'demo_mode': DEMO_MODE
        })

    except Exception as e:
        print(f"Upload error: {str(e)}")
        return jsonify({'error': f'L·ªói: {str(e)}'}), 500

def list_files_only_recursive(bucket_name, folder_path=""):
    """
    Ch·ªâ l·∫•y danh s√°ch FILE, KH√îNG bao g·ªìm folder
    ƒê∆∞·ª£c thi·∫øt k·∫ø ƒë·∫∑c bi·ªát ƒë·ªÉ tr√°nh x√≥a nh·∫ßm folder
    """
    if DEMO_MODE:
        return []
    
    try:
        files_only = []
        
        # L·∫•y danh s√°ch items trong folder hi·ªán t·∫°i
        result = supabase.storage.from_(bucket_name).list(folder_path)
        
        if not result:
            return files_only
            
        for item in result:
            item_name = item.get('name', '')
            if not item_name:
                continue
                
            # T·∫°o full path
            if folder_path:
                full_path = f"{folder_path}/{item_name}"
            else:   
                full_path = item_name
            
            # LOGIC CH·∫∂T CH·∫º ƒë·ªÉ ch·ªâ l·∫•y FILE
            item_id = item.get('id')
            item_size = item.get('size')
            item_mimetype = item.get('mimetype') 
            
            # File ph·∫£i c√≥ √≠t nh·∫•t 1 trong c√°c ƒë·∫∑c ƒëi·ªÉm n√†y:
            # 1. C√≥ ID (Supabase t·ª± generate cho file)
            # 2. C√≥ size > 0 
            # 3. C√≥ mimetype
            # 4. C√≥ extension r√µ r√†ng
            has_clear_extension = (
                '.' in item_name and 
                not item_name.startswith('.') and
                len(item_name.split('.')[-1]) <= 10  # Extension h·ª£p l·ªá
            )
            
            is_definitely_file = (
                (item_id is not None) or 
                (item_size is not None and item_size > 0) or 
                (item_mimetype is not None and item_mimetype.strip() != '') or
                has_clear_extension
            )
            
            # QUAN TR·ªåNG: Ch·ªâ x·ª≠ l√Ω n·∫øu ch·∫Øc ch·∫Øn l√† FILE
            if is_definitely_file:
                files_only.append({
                    'name': item_name,
                    'path': full_path,
                    'type': 'file',
                    'size': item_size or 0,
                    'mimetype': item_mimetype or '',
                    'last_modified': item.get('updated_at', item.get('created_at', '')),
                    'metadata': item
                })
                logger.debug(f"‚úì Detected FILE: {full_path} (size: {item_size}, mimetype: {item_mimetype})")
            else:
                # ƒê√¢y c√≥ th·ªÉ l√† folder - ƒê·ªá quy ƒë·ªÉ t√¨m file b√™n trong
                # NH∆ØNG KH√îNG th√™m folder v√†o danh s√°ch
                logger.debug(f"üìÅ Detected FOLDER: {full_path} - exploring inside...")
                try:
                    subfolder_files = list_files_only_recursive(bucket_name, full_path)
                    files_only.extend(subfolder_files)
                except Exception as subfolder_error:
                    logger.warning(f"Cannot explore subfolder {full_path}: {str(subfolder_error)}")
        
        return files_only
        
    except Exception as e:
        logger.error(f"Error listing files in {folder_path}: {str(e)}")
        return []

def delete_only_files_safe(bucket_name, file_items, batch_size=5):
    """
    An to√†n ch·ªâ x√≥a FILE, tuy·ªát ƒë·ªëi kh√¥ng x√≥a folder
    """
    if DEMO_MODE:
        return {'deleted': len(file_items), 'failed': 0, 'errors': []}
    
    deleted_count = 0
    failed_count = 0
    errors = []
    
    logger.info(f"üéØ Starting SAFE file deletion for {len(file_items)} files...")
    
    # X·ª≠ l√Ω t·ª´ng file m·ªôt c√°ch c·∫©n th·∫≠n
    for i, file_item in enumerate(file_items):
        file_path = file_item['path']
        file_name = file_item['name']
        
        try:
            # KI·ªÇM TRA CU·ªêI C√ôNG: ƒê√¢y c√≥ ph·∫£i file kh√¥ng?
            if not file_name or '.' not in file_name:
                logger.warning(f"‚ö†Ô∏è SKIP suspicious path (no extension): {file_path}")
                continue
                
            # Ki·ªÉm tra extension h·ª£p l·ªá
            extension = file_name.split('.')[-1].lower()
            valid_extensions = [
                'jpg', 'jpeg', 'png', 'gif', 'bmp', 'webp', 'svg',  # Images
                'pdf', 'doc', 'docx', 'txt', 'rtf',  # Documents  
                'xls', 'xlsx', 'csv',  # Spreadsheets
                'mp4', 'avi', 'mov', 'wmv', 'flv',  # Videos
                'mp3', 'wav', 'flac', 'aac',  # Audio
                'zip', 'rar', '7z', 'tar', 'gz',  # Archives
                'json', 'xml', 'html', 'css', 'js', 'py', 'java', 'cpp'  # Code
            ]
            
            if extension not in valid_extensions:
                logger.warning(f"‚ö†Ô∏è SKIP unknown extension: {file_path} (.{extension})")
                continue
            
            logger.info(f"üóëÔ∏è Deleting file [{i+1}/{len(file_items)}]: {file_path}")
            
            # Th·ª±c hi·ªán x√≥a file
            result = supabase.storage.from_(bucket_name).remove([file_path])
            
            if result:
                deleted_count += 1
                logger.info(f"‚úÖ Successfully deleted: {file_path}")
            else:
                deleted_count += 1  # Coi nh∆∞ th√†nh c√¥ng n·∫øu kh√¥ng c√≥ error
                logger.info(f"‚úÖ File processed (may already be deleted): {file_path}")
                
        except Exception as file_error:
            error_str = str(file_error).lower()
            if any(keyword in error_str for keyword in ['not found', 'does not exist', 'not exist']):
                deleted_count += 1
                logger.info(f"‚úÖ File already deleted: {file_path}")
            else:
                failed_count += 1
                error_msg = f"‚ùå Failed to delete {file_path}: {str(file_error)}"
                errors.append(error_msg)
                logger.error(error_msg)
        
        # Delay ƒë·ªÉ tr√°nh rate limit
        if (i + 1) % batch_size == 0 and (i + 1) < len(file_items):
            time.sleep(1)
            logger.info(f"‚è≥ Processed {i + 1}/{len(file_items)} files...")
    
    return {
        'deleted': deleted_count,
        'failed': failed_count,
        'errors': errors
    }

@app.route('/api/storage/cleanup-files-only', methods=['POST'])
def cleanup_files_only():
    """API ƒë·ªÉ CH·ªà X√ìA FILE - TUY·ªÜT ƒê·ªêI KH√îNG X√ìA TH∆Ø M·ª§C"""
    try:
        if DEMO_MODE:
            return jsonify({
                'success': True,
                'message': 'üîß Demo mode: M√¥ ph·ªèng x√≥a files (th∆∞ m·ª•c ƒë∆∞·ª£c b·∫£o v·ªá)',
                'demo_mode': True,
                'total_files': 0,
                'deleted': 0,
                'failed': 0,
                'errors': []
            })
        
        client_ip = get_client_ip()
        logger.warning(f"üõ°Ô∏è SAFE FILES-ONLY CLEANUP initiated from IP: {client_ip}")
        
        # S·ª≠ d·ª•ng h√†m m·ªõi ch·ªâ l·∫•y file
        logger.info("üìã Scanning for FILES ONLY (folders are protected)...")
        file_items = list_files_only_recursive(SUPABASE_BUCKET)
        
        if not file_items:
            return jsonify({
                'success': True,
                'message': '‚úÖ Kh√¥ng t√¨m th·∫•y file n√†o ƒë·ªÉ x√≥a (th∆∞ m·ª•c ƒë∆∞·ª£c b·∫£o v·ªá)',
                'total_files': 0,
                'deleted': 0,
                'failed': 0,
                'errors': []
            })
        
        total_files = len(file_items)
        total_size = sum(item.get('size', 0) for item in file_items)
        
        logger.info(f"üìä Found {total_files} FILES to delete")
        logger.info(f"üì¶ Total size: {format_file_size(total_size)}")
        logger.info(f"üõ°Ô∏è ALL FOLDERS WILL BE PROTECTED")
        
        # Log m·ªôt v√†i file ƒë·∫ßu ƒë·ªÉ ki·ªÉm tra
        logger.info("üìã Sample files to delete:")
        for i, item in enumerate(file_items[:5]):
            logger.info(f"  {i+1}. {item['path']} ({format_file_size(item.get('size', 0))})")
        if len(file_items) > 5:
            logger.info(f"  ... and {len(file_items) - 5} more files")
        
        # Th·ª±c hi·ªán x√≥a AN TO√ÄN
        logger.info("üöÄ Starting PROTECTED file deletion...")
        delete_result = delete_only_files_safe(SUPABASE_BUCKET, file_items)
        
        # X√≥a database records (optional)
        db_deleted = 0
        try:
            db_result = supabase.table('submissions').delete().neq('id', 0).execute()
            db_deleted = len(db_result.data) if db_result.data else 0
            logger.info(f"üóÑÔ∏è Cleaned {db_deleted} database records")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Database cleanup failed: {str(e)}")
        
        # K·∫øt qu·∫£
        success_rate = (delete_result['deleted'] / total_files * 100) if total_files > 0 else 100
        
        message = f"üõ°Ô∏è ƒê√É X√ìA {delete_result['deleted']}/{total_files} FILE - TH∆Ø M·ª§C ƒê∆Ø·ª¢C B·∫¢O V·ªÜ"
        if delete_result['failed'] > 0:
            message += f" (‚ùå {delete_result['failed']} l·ªói)"
        if db_deleted > 0:
            message += f" (üóÑÔ∏è DB: {db_deleted} records)"
        
        logger.warning(f"üõ°Ô∏è PROTECTED CLEANUP COMPLETED: {message}")
        
        return jsonify({
            'success': True,
            'message': message,
            'total_files': total_files,
            'total_size': total_size,
            'total_size_human': format_file_size(total_size),
            'deleted': delete_result['deleted'],
            'failed': delete_result['failed'],
            'database_records_deleted': db_deleted,
            'success_rate': round(success_rate, 2),
            'errors': delete_result['errors'][:10],
            'client_ip': client_ip,
            'cleanup_type': 'files_only_protected',
            'folders_protected': True,
            'note': 'üõ°Ô∏è T·∫•t c·∫£ th∆∞ m·ª•c ƒë∆∞·ª£c b·∫£o v·ªá ho√†n to√†n - ch·ªâ x√≥a file'
        })
        
    except Exception as e:
        error_msg = f"üõ°Ô∏è Protected cleanup error: {str(e)}"
        logger.error(error_msg)
        return jsonify({
            'success': False,
            'error': error_msg
        }), 500

def list_all_items_with_type(bucket_name, folder_path=""):
    """
    L·∫•y t·∫•t c·∫£ items v√† ph√¢n lo·∫°i r√µ r√†ng file vs folder
    D√πng cho storage info v√† full cleanup
    """
    if DEMO_MODE:
        return []
    
    try:
        all_items = []
        
        result = supabase.storage.from_(bucket_name).list(folder_path)
        if not result:
            return all_items
            
        for item in result:
            item_name = item.get('name', '')
            if not item_name:
                continue
                
            if folder_path:
                full_path = f"{folder_path}/{item_name}"
            else:   
                full_path = item_name
            
            # Ph√¢n lo·∫°i d·ª±a tr√™n metadata Supabase
            item_id = item.get('id')
            item_size = item.get('size')
            item_mimetype = item.get('mimetype')
            
            # File: c√≥ metadata r√µ r√†ng ho·∫∑c c√≥ extension
            is_file = (
                item_id is not None or 
                (item_size is not None and item_size > 0) or 
                (item_mimetype is not None and item_mimetype.strip() != '') or
                ('.' in item_name and not item_name.startswith('.') and not item_name.endswith('/'))
            )
            
            if is_file:
                all_items.append({
                    'name': item_name,
                    'path': full_path,
                    'type': 'file',
                    'size': item_size or 0,
                    'mimetype': item_mimetype or '',
                    'last_modified': item.get('updated_at', item.get('created_at', '')),
                    'metadata': item
                })
                logger.debug(f"‚úì Detected FILE: {full_path}")
            else:
                # Folder - th√™m v√†o danh s√°ch
                all_items.append({
                    'name': item_name,
                    'path': full_path,
                    'type': 'folder',
                    'size': 0,
                    'last_modified': item.get('updated_at', item.get('created_at', '')),
                    'metadata': item
                })
                logger.debug(f"üìÅ Detected FOLDER: {full_path}")
                
                # ƒê·ªá quy v√†o folder
                try:
                    subfolder_items = list_all_items_with_type(bucket_name, full_path)
                    all_items.extend(subfolder_items)
                except Exception as e:
                    logger.warning(f"Cannot access subfolder {full_path}: {str(e)}")
        
        return all_items
        
    except Exception as e:
        logger.error(f"Error listing items in {folder_path}: {str(e)}")
        return []

def delete_items_batch(bucket_name, items_to_delete, batch_size=10):
    """
    X√≥a items theo batch ƒë·ªÉ tr√°nh timeout v√† rate limit
    """
    if not items_to_delete:
        return {'deleted': 0, 'failed': 0, 'errors': []}
    
    deleted_count = 0
    failed_count = 0
    errors = []
    
    # Chia th√†nh c√°c batch nh·ªè
    for i in range(0, len(items_to_delete), batch_size):
        batch = items_to_delete[i:i + batch_size]
        batch_paths = [item['path'] for item in batch]
        
        try:
            logger.info(f"üóëÔ∏è Deleting batch {i//batch_size + 1}: {len(batch_paths)} items")
            logger.debug(f"Batch paths: {batch_paths}")
            
            # X√≥a theo batch
            result = supabase.storage.from_(bucket_name).remove(batch_paths)
            
            # Ki·ªÉm tra k·∫øt qu·∫£
            if result:
                # N·∫øu c√≥ k·∫øt qu·∫£ tr·∫£ v·ªÅ, ki·ªÉm tra t·ª´ng item
                if isinstance(result, list):
                    for idx, res in enumerate(result):
                        if res.get('error'):
                            error_msg = res.get('error', {}).get('message', 'Unknown error')
                            if 'not found' in error_msg.lower() or 'does not exist' in error_msg.lower():
                                deleted_count += 1
                                logger.info(f"‚úÖ Item already deleted: {batch_paths[idx]}")
                            else:
                                failed_count += 1
                                errors.append(f"‚ùå {batch_paths[idx]}: {error_msg}")
                                logger.error(f"‚ùå Failed to delete {batch_paths[idx]}: {error_msg}")
                        else:
                            deleted_count += 1
                            logger.info(f"‚úÖ Successfully deleted: {batch_paths[idx]}")
                else:
                    # N·∫øu kh√¥ng c√≥ error, coi nh∆∞ t·∫•t c·∫£ ƒë√£ x√≥a th√†nh c√¥ng
                    deleted_count += len(batch_paths)
                    logger.info(f"‚úÖ Batch deleted successfully: {len(batch_paths)} items")
            else:
                # N·∫øu kh√¥ng c√≥ result, coi nh∆∞ th√†nh c√¥ng
                deleted_count += len(batch_paths)
                logger.info(f"‚úÖ Batch processed: {len(batch_paths)} items")
                
        except Exception as e:
            error_str = str(e).lower()
            # X·ª≠ l√Ω t·ª´ng item trong batch khi c√≥ l·ªói
            for path in batch_paths:
                if any(keyword in error_str for keyword in ['not found', 'does not exist', 'not exist']):
                    deleted_count += 1
                    logger.info(f"‚úÖ Item already deleted: {path}")
                else:
                    failed_count += 1
                    error_msg = f"‚ùå Failed to delete {path}: {str(e)}"
                    errors.append(error_msg)
                    logger.error(error_msg)
        
        # Delay gi·ªØa c√°c batch
        if i + batch_size < len(items_to_delete):
            time.sleep(0.5)
            logger.info(f"‚è≥ Processed {min(i + batch_size, len(items_to_delete))}/{len(items_to_delete)} items...")
    
    return {
        'deleted': deleted_count,
        'failed': failed_count,
        'errors': errors
    }
@app.route('/api/storage/cleanup', methods=['POST'])
def cleanup_storage():
    """API ƒë·ªÉ d·ªçn d·∫πp t·∫•t c·∫£ file V√Ä th∆∞ m·ª•c - FIXED VERSION"""
    try:
        if DEMO_MODE:
            return jsonify({
                'success': True,
                'message': 'üîß Demo mode: M√¥ ph·ªèng x√≥a t·∫•t c·∫£',
                'demo_mode': True,
                'total_items': 0,
                'deleted': 0,
                'failed': 0,
                'errors': []
            })
        
        client_ip = get_client_ip()
        logger.warning(f"üí• FULL STORAGE CLEANUP initiated from IP: {client_ip}")
        
        # L·∫•y t·∫•t c·∫£ items
        logger.info("üìã Scanning ALL items (files + folders)...")
        all_items = list_all_items_with_type(SUPABASE_BUCKET)
        
        if not all_items:
            return jsonify({
                'success': True,
                'message': '‚úÖ Storage ƒë√£ tr·ªëng',
                'total_items': 0,
                'deleted': 0,
                'failed': 0,
                'errors': []
            })
        
        # Ph√¢n lo·∫°i
        files = [item for item in all_items if item['type'] == 'file']
        folders = [item for item in all_items if item['type'] == 'folder']
        
        total_files = len(files)
        total_folders = len(folders)
        total_size = sum(item.get('size', 0) for item in files)
        
        logger.info(f"üí• FULL CLEANUP TARGET:")
        logger.info(f"   üìÑ Files: {total_files}")
        logger.info(f"   üìÅ Folders: {total_folders}")
        logger.info(f"   üíæ Total size: {format_file_size(total_size)}")
        
        # Log sample items
        logger.info("üìã Sample items to delete:")
        sample_files = files[:3]
        sample_folders = folders[:3]
        for i, item in enumerate(sample_files):
            logger.info(f"  üìÑ {i+1}. {item['path']} ({format_file_size(item.get('size', 0))})")
        for i, item in enumerate(sample_folders):
            logger.info(f"  üìÅ {i+1}. {item['path']}")
        
        # STRATEGY: X√≥a T·∫§T C·∫¢ c√πng l√∫c (files + folders)
        # S·∫Øp x·∫øp theo ƒë·ªô s√¢u (deep first) ƒë·ªÉ tr√°nh l·ªói folder kh√¥ng r·ªóng
        all_items_sorted = sorted(all_items, key=lambda x: (
            x['path'].count('/'),  # ƒê·ªô s√¢u
            x['type'] == 'folder'  # File tr∆∞·ªõc, folder sau
        ), reverse=True)
        
        logger.info(f"üöÄ Starting FULL deletion of {len(all_items_sorted)} items...")
        logger.info("üìã Deletion order (deep-first):")
        for i, item in enumerate(all_items_sorted[:5]):
            logger.info(f"  {i+1}. [{item['type'].upper()}] {item['path']}")
        
        # Th·ª±c hi·ªán x√≥a
        delete_result = delete_items_batch(SUPABASE_BUCKET, all_items_sorted, batch_size=8)
        
        # Database cleanup
        db_deleted = 0
        try:
            logger.info("üóÑÔ∏è Cleaning database records...")
            db_result = supabase.table('submissions').delete().neq('id', 0).execute()
            db_deleted = len(db_result.data) if db_result.data else 0
            logger.info(f"üóÑÔ∏è Cleaned {db_deleted} database records")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Database cleanup failed: {str(e)}")
        
        # K·∫øt qu·∫£
        total_items = total_files + total_folders
        success_rate = (delete_result['deleted'] / total_items * 100) if total_items > 0 else 100
        
        message = f"üí• X√ìA {delete_result['deleted']}/{total_items} items"
        message += f" ({total_files} files + {total_folders} folders)"
        if delete_result['failed'] > 0:
            message += f" (‚ùå {delete_result['failed']} l·ªói)"
        if db_deleted > 0:
            message += f" (üóÑÔ∏è DB: {db_deleted})"
        
        logger.warning(f"üí• FULL CLEANUP COMPLETED: {message}")
        logger.info(f"üìä Success rate: {success_rate:.1f}%")
        
        return jsonify({
            'success': True,
            'message': message,
            'total_items': total_items,
            'total_files': total_files,
            'total_folders': total_folders,
            'total_size': total_size,
            'total_size_human': format_file_size(total_size),
            'deleted': delete_result['deleted'],
            'failed': delete_result['failed'],
            'database_records_deleted': db_deleted,
            'success_rate': round(success_rate, 2),
            'errors': delete_result['errors'][:15],
            'client_ip': client_ip,
            'cleanup_type': 'full',
            'strategy': 'deep_first_batch_delete'
        })
        
    except Exception as e:
        error_msg = f"üí• Full cleanup error: {str(e)}"
        logger.error(error_msg)
        logger.exception("Full cleanup exception details:")
        return jsonify({
            'success': False,
            'error': error_msg,
            'cleanup_type': 'full'
        }), 500


@app.route('/api/storage/info', methods=['GET'])
def storage_info():
    """Th√¥ng tin storage v·ªõi ph√¢n lo·∫°i ch√≠nh x√°c"""
    try:
        if DEMO_MODE:
            return jsonify({
                'success': True,
                'demo_mode': True,
                'total_files': 0,
                'total_folders': 0,
                'total_size': 0,
                'message': 'üîß Demo mode'
            })

        all_items = list_all_items_with_type(SUPABASE_BUCKET)
        
        files = [item for item in all_items if item['type'] == 'file']
        folders = [item for item in all_items if item['type'] == 'folder']

        total_files = len(files)
        total_folders = len(folders)
        total_size = sum(file.get('size', 0) for file in files)

        # Folder stats
        folder_stats = {}
        for file in files:
            folder_name = file['path'].split('/')[0] if '/' in file['path'] else 'root'
            if folder_name not in folder_stats:
                folder_stats[folder_name] = {'count': 0, 'size': 0}
            folder_stats[folder_name]['count'] += 1
            folder_stats[folder_name]['size'] += file.get('size', 0)

        folder_list = [
            {
                'name': name,
                'file_count': stats['count'],
                'total_size': stats['size'],
                'size_human': format_file_size(stats['size'])
            }
            for name, stats in folder_stats.items()
        ]
        folder_list.sort(key=lambda x: x['total_size'], reverse=True)

        # File types
        file_types = {}
        for file in files:
            if '.' in file['path']:
                ext = file['path'].split('.')[-1].lower()
                if ext not in file_types:
                    file_types[ext] = {'count': 0, 'size': 0}
                file_types[ext]['count'] += 1
                file_types[ext]['size'] += file.get('size', 0)

        return jsonify({
            'success': True,
            'total_files': total_files,
            'total_folders': total_folders,
            'total_items': total_files + total_folders,
            'total_size': total_size,
            'total_size_human': format_file_size(total_size),
            'folder_stats': folder_list,
            'bucket': SUPABASE_BUCKET,
            'recent_files': sorted(files, key=lambda x: x.get('last_modified', ''), reverse=True)[:10],
            'file_types': file_types,
            'cleanup_options': {
                'files_only': 'üõ°Ô∏è X√≥a ch·ªâ file - B·∫£o v·ªá th∆∞ m·ª•c',
                'full': 'üí• X√≥a t·∫•t c·∫£ file v√† th∆∞ m·ª•c'
            }
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/storage/cleanup-files-confirm', methods=['POST']) 
def cleanup_files_confirm():
    """X√°c nh·∫≠n tr∆∞·ªõc khi x√≥a ch·ªâ file (B·∫¢O V·ªÜ th∆∞ m·ª•c)"""
    try:
        confirm_code = request.json.get('confirm_code', '') if request.is_json else ''
        expected_code = "DELETE_FILES_KEEP_FOLDERS"
        
        if confirm_code != expected_code:
            return jsonify({
                'success': False,
                'error': f'‚ùå Nh·∫≠p m√£ x√°c nh·∫≠n: "{expected_code}"',
                'required_code': expected_code,
                'note': 'üõ°Ô∏è Thao t√°c n√†y CH·ªà x√≥a file - TH∆Ø M·ª§C ƒê∆Ø·ª¢C B·∫¢O V·ªÜ'
            }), 400
        
        logger.info("üõ°Ô∏è Confirmed: FILES-ONLY cleanup (folders protected)")
        return cleanup_files_only()
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/storage/cleanup-confirm', methods=['POST']) 
def cleanup_confirm():
    """X√°c nh·∫≠n tr∆∞·ªõc khi x√≥a t·∫•t c·∫£"""
    try:
        confirm_code = request.json.get('confirm_code', '') if request.is_json else ''
        expected_code = "DELETE_ALL_FILES_AND_FOLDERS"
        
        if confirm_code != expected_code:
            return jsonify({
                'success': False,
                'error': f'‚ùå Nh·∫≠p m√£ x√°c nh·∫≠n: "{expected_code}"',
                'required_code': expected_code,
                'note': 'üí• Thao t√°c n√†y X√ìA T·∫§T C·∫¢'
            }), 400
        
        logger.info("üí• Confirmed: FULL cleanup")
        return cleanup_storage()
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500
def get_all_storage_files(supabase, bucket_name, path="", max_files=5000):
    """
    L·∫•y t·∫•t c·∫£ files trong storage m·ªôt c√°ch recursive
    """
    all_files = []
    folders_to_process = [path] if path else [""]
    processed_folders = set()
    
    while folders_to_process:
        current_path = folders_to_process.pop(0)
        
        # Tr√°nh x·ª≠ l√Ω folder tr√πng l·∫∑p
        if current_path in processed_folders:
            continue
        processed_folders.add(current_path)
        
        try:
            # L·∫•y items trong folder hi·ªán t·∫°i
            items = supabase.storage.from_(bucket_name).list(
                path=current_path, 
                options={"limit": 1000}
            )
            
            if not items:
                continue
                
            for item in items:
                item_name = item.get('name', '')
                if not item_name:
                    continue
                    
                # T·∫°o full path
                if current_path:
                    full_path = f"{current_path}/{item_name}"
                else:
                    full_path = item_name
                
                # Ki·ªÉm tra xem ƒë√¢y l√† file hay folder
                # Folder th∆∞·ªùng c√≥ size = 0 ho·∫∑c None v√† updated_at = None
                metadata = item.get('metadata', {}) or {}
                file_size = metadata.get('size', 0) or 0
                updated_at = item.get('updated_at')
                
                # N·∫øu l√† folder (size = 0 v√† kh√¥ng c√≥ updated_at)
                if file_size == 0 and updated_at is None:
                    # Th√™m folder v√†o queue ƒë·ªÉ x·ª≠ l√Ω
                    folders_to_process.append(full_path)
                else:
                    # ƒê√¢y l√† file th·∫≠t
                    all_files.append({
                        'name': item_name,
                        'full_path': full_path,
                        'folder': current_path,
                        'size': file_size,
                        'updated_at': updated_at,
                        'metadata': item
                    })
                
                # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng files ƒë·ªÉ tr√°nh timeout
                if len(all_files) >= max_files:
                    break
            
            if len(all_files) >= max_files:
                break
                
        except Exception as e:
            print(f"L·ªói khi l·∫•y files t·ª´ folder '{current_path}': {str(e)}")
            continue
    
    return all_files

# API ƒë·ªÉ l·∫•y preview c·∫•u tr√∫c th∆∞ m·ª•c tr∆∞·ªõc khi download (FIXED)
@app.route('/api/preview/storage-structure', methods=['GET'])
def preview_storage_structure():
    """
    API ƒë·ªÉ xem preview c·∫•u tr√∫c storage tr∆∞·ªõc khi download
    """
    try:
        if DEMO_MODE:
            return jsonify({
                'success': False,
                'error': 'T√≠nh nƒÉng n√†y kh√¥ng kh·∫£ d·ª•ng ·ªü Demo Mode'
            }), 400
        
        print("üîç ƒêang qu√©t c·∫•u tr√∫c storage...")
        
        # L·∫•y t·∫•t c·∫£ files t·ª´ storage
        all_files = get_all_storage_files(supabase, SUPABASE_BUCKET)
        
        if not all_files:
            return jsonify({
                'success': True,
                'message': 'Kh√¥ng t√¨m th·∫•y files n√†o trong storage',
                'data': {
                    'folders': {},
                    'root_files': [],
                    'statistics': {
                        'total_folders': 0,
                        'total_files': 0,
                        'total_size': 0,
                        'total_size_human': '0 B'
                    }
                }
            })
        
        # Ph√¢n t√≠ch c·∫•u tr√∫c
        folders_structure = {}
        root_files = []
        total_size = 0
        
        for file_info in all_files:
            file_name = file_info['name']
            full_path = file_info['full_path']
            folder = file_info['folder']
            file_size = file_info['size']
            updated_at = file_info['updated_at']
            
            total_size += file_size
            
            if folder and folder != "":
                # File trong subfolder
                if folder not in folders_structure:
                    folders_structure[folder] = {
                        'name': folder,
                        'files': [],
                        'file_count': 0,
                        'total_size': 0
                    }
                
                folders_structure[folder]['files'].append({
                    'name': file_name,
                    'full_path': full_path,
                    'size': file_size,
                    'size_human': format_file_size(file_size),
                    'updated_at': updated_at
                })
                folders_structure[folder]['file_count'] += 1
                folders_structure[folder]['total_size'] += file_size
                
            else:
                # File ·ªü root
                root_files.append({
                    'name': file_name,
                    'full_path': full_path,
                    'size': file_size,
                    'size_human': format_file_size(file_size),
                    'updated_at': updated_at
                })
        
        # Format folders
        for folder_name, folder_info in folders_structure.items():
            folder_info['total_size_human'] = format_file_size(folder_info['total_size'])
        
        # T√≠nh to√°n th·ªëng k√™
        total_files = len(all_files)
        
        print(f"‚úÖ Qu√©t ho√†n th√†nh: {total_files} files trong {len(folders_structure)} folders")
        
        return jsonify({
            'success': True,
            'data': {
                'folders': folders_structure,
                'root_files': root_files,
                'statistics': {
                    'total_folders': len(folders_structure),
                    'total_files': total_files,
                    'total_size': total_size,
                    'total_size_human': format_file_size(total_size),
                    'folders_with_files': len([f for f in folders_structure.values() if f['file_count'] > 0]),
                    'root_file_count': len(root_files)
                }
            }
        })
        
    except Exception as e:
        print(f"‚ùå L·ªói l·∫•y c·∫•u tr√∫c storage: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'L·ªói l·∫•y c·∫•u tr√∫c storage: {str(e)}'
        }), 500
@app.route('/api/download/all-folders', methods=['GET', 'POST'])
def download_all_folders():
    """
    API ƒë·ªÉ download t·∫•t c·∫£ folders v√† files t·ª´ Supabase Storage v·ªÅ m√°y local
    H·ªó tr·ª£ incremental sync - ch·ªâ t·∫£i file m·ªõi/thay ƒë·ªïi t·ª´ l·∫ßn 2
    
    Query Parameters:
    - format: 'zip' ho·∫∑c 'folders' (default: 'folders')
    - path: ƒë∆∞·ªùng d·∫´n l∆∞u local (default: './downloads')
    - include_metadata: true/false - c√≥ l∆∞u metadata kh√¥ng (default: true)
    - force_full: true/false - bu·ªôc download to√†n b·ªô (default: false)
    - sync_mode: 'incremental' ho·∫∑c 'full' (default: 'incremental')
    """
    try:
        if DEMO_MODE:
            return jsonify({
                'success': False,
                'error': 'T√≠nh nƒÉng n√†y kh√¥ng kh·∫£ d·ª•ng ·ªü Demo Mode'
            }), 400
        
        # L·∫•y parameters
        download_format = request.args.get('format', 'folders').lower()
        local_path = request.args.get('path', './downloads')
        include_metadata = request.args.get('include_metadata', 'true').lower() == 'true'
        force_full = request.args.get('force_full', 'false').lower() == 'true'
        sync_mode = request.args.get('sync_mode', 'incremental').lower()
        
        # T·∫°o th∆∞ m·ª•c download n·∫øu ch∆∞a c√≥
        download_dir = Path(local_path)
        download_dir.mkdir(parents=True, exist_ok=True)
        
        # ƒê∆∞·ªùng d·∫´n file sync state
        sync_state_file = download_dir / '.sync_state.json'
        
        print(f"üöÄ B·∫Øt ƒë·∫ßu {'full' if force_full or sync_mode == 'full' else 'incremental'} download...")
        print(f"üìÅ L∆∞u t·∫°i: {download_dir.absolute()}")
        print(f"üì¶ Format: {download_format}")
        
        # ƒê·ªçc sync state t·ª´ l·∫ßn download tr∆∞·ªõc (n·∫øu c√≥)
        previous_sync_state = {}
        is_first_sync = True
        
        if sync_state_file.exists() and not force_full and sync_mode == 'incremental':
            try:
                with open(sync_state_file, 'r', encoding='utf-8') as f:
                    previous_sync_state = json.load(f)
                is_first_sync = False
                print(f"üìã T√¨m th·∫•y sync state t·ª´ l·∫ßn tr∆∞·ªõc: {previous_sync_state.get('last_sync', 'N/A')}")
            except Exception as e:
                print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ ƒë·ªçc sync state: {e}. S·∫Ω th·ª±c hi·ªán full sync.")
                is_first_sync = True
        
        # L·∫•y t·∫•t c·∫£ files t·ª´ storage
        try:
            all_files = get_all_storage_files(supabase, SUPABASE_BUCKET)
            
            if not all_files:
                return jsonify({
                    'success': False,
                    'error': 'Kh√¥ng t√¨m th·∫•y file n√†o trong storage'
                }), 404
            
            print(f"üìä T√¨m th·∫•y {len(all_files)} files tr√™n storage")
            
        except Exception as e:
            return jsonify({
                'success': False,
                'error': f'L·ªói l·∫•y danh s√°ch files: {str(e)}'
            }), 500
        
        # L·∫•y danh s√°ch folders t·ª´ storage ƒë·ªÉ t·∫°o c·∫•u tr√∫c th∆∞ m·ª•c
        folders_on_storage = set()
        files_to_process = []
        
        # Ph√¢n lo·∫°i files v√† x√°c ƒë·ªãnh files c·∫ßn download
        for file_info in all_files:
            folder = file_info['folder']
            file_path = file_info['full_path']
            
            # Th√™m folder v√†o danh s√°ch
            if folder and folder != "":
                folders_on_storage.add(folder)
                # Th√™m c√°c parent folders n·∫øu c√≥ nested structure
                folder_parts = folder.split('/')
                for i in range(1, len(folder_parts) + 1):
                    parent_folder = '/'.join(folder_parts[:i])
                    folders_on_storage.add(parent_folder)
            
            # Ki·ªÉm tra xem file c√≥ c·∫ßn download kh√¥ng
            should_download = True
            
            if not is_first_sync and sync_mode == 'incremental':
                # So s√°nh v·ªõi sync state tr∆∞·ªõc
                previous_file_info = previous_sync_state.get('files', {}).get(file_path)
                
                if previous_file_info:
                    # So s√°nh updated_at v√† size
                    current_updated = file_info.get('updated_at', '')
                    previous_updated = previous_file_info.get('updated_at', '')
                    current_size = file_info.get('size', 0)
                    previous_size = previous_file_info.get('size', 0)
                    
                    if (current_updated == previous_updated and 
                        current_size == previous_size):
                        # File kh√¥ng thay ƒë·ªïi, ki·ªÉm tra xem file local c√≥ t·ªìn t·∫°i kh√¥ng
                        if folder:
                            local_file_path = download_dir / folder / file_info['name']
                        else:
                            local_file_path = download_dir / file_info['name']
                        
                        if local_file_path.exists():
                            should_download = False
                            print(f"‚è≠Ô∏è Skip unchanged file: {file_path}")
            
            if should_download:
                files_to_process.append(file_info)
        
        print(f"üìÅ T√¨m th·∫•y {len(folders_on_storage)} folders tr√™n storage")
        print(f"üìÑ C·∫ßn download {len(files_to_process)} files")
        
        # T·∫°o t·∫•t c·∫£ folders tr∆∞·ªõc khi download
        print("üèóÔ∏è T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c...")
        for folder_name in sorted(folders_on_storage):
            folder_path = download_dir / folder_name
            folder_path.mkdir(parents=True, exist_ok=True)
            print(f"   üìÅ Created: {folder_path}")
        
        # N·∫øu kh√¥ng c√≥ file n√†o c·∫ßn download
        if not files_to_process:
            print("‚úÖ T·∫•t c·∫£ files ƒë√£ ƒë∆∞·ª£c sync, kh√¥ng c√≥ g√¨ ƒë·ªÉ download!")
            
            return jsonify({
                'success': True,
                'message': 'T·∫•t c·∫£ files ƒë√£ ƒë∆∞·ª£c sync, kh√¥ng c√≥ file m·ªõi ƒë·ªÉ download',
                'data': {
                    'download_path': str(download_dir.absolute()),
                    'sync_type': 'incremental',
                    'statistics': {
                        'total_folders': len(folders_on_storage),
                        'total_files_on_storage': len(all_files),
                        'files_to_download': 0,
                        'already_synced': len(all_files),
                        'total_size': 0
                    }
                }
            })
        
        # Ph√¢n lo·∫°i files c·∫ßn download theo folders
        folders_structure = {}
        root_files = []
        
        for file_info in files_to_process:
            folder = file_info['folder']
            
            if folder and folder != "":
                if folder not in folders_structure:
                    folders_structure[folder] = []
                folders_structure[folder].append(file_info)
            else:
                root_files.append(file_info)
        
        download_results = {
            'folders': {},
            'root_files': [],
            'total_files': 0,
            'total_size': 0,
            'errors': [],
            'skipped_files': len(all_files) - len(files_to_process)
        }
        
        # Function ƒë·ªÉ t√≠nh checksum c·ªßa file
        def calculate_file_checksum(file_path):
            try:
                hash_md5 = hashlib.md5()
                with open(file_path, "rb") as f:
                    for chunk in iter(lambda: f.read(4096), b""):
                        hash_md5.update(chunk)
                return hash_md5.hexdigest()
            except:
                return None
        
        # Function ƒë·ªÉ download m·ªôt file
        def download_single_file(file_info, local_file_path):
            try:
                storage_path = file_info['full_path']
                
                # Download file t·ª´ Supabase
                file_data = supabase.storage.from_(SUPABASE_BUCKET).download(storage_path)
                
                if not file_data:
                    return {
                        'success': False,
                        'error': f'Kh√¥ng th·ªÉ download {storage_path}',
                        'path': storage_path
                    }
                
                # T·∫°o th∆∞ m·ª•c n·∫øu c·∫ßn (ƒë√£ t·∫°o tr∆∞·ªõc ƒë√≥ nh∆∞ng ƒë·∫£m b·∫£o)
                local_file_path.parent.mkdir(parents=True, exist_ok=True)
                
                # Ghi file
                with open(local_file_path, 'wb') as f:
                    f.write(file_data)
                
                file_size = len(file_data)
                
                # T√≠nh checksum ƒë·ªÉ verify
                checksum = calculate_file_checksum(local_file_path)
                
                return {
                    'success': True,
                    'storage_path': storage_path,
                    'local_path': str(local_file_path),
                    'size': file_size,
                    'checksum': checksum,
                    'metadata': file_info
                }
                
            except Exception as e:
                return {
                    'success': False,
                    'error': str(e),
                    'path': file_info['full_path']
                }
        
        # T·∫°o current sync state ƒë·ªÉ l∆∞u
        current_sync_state = {
            'last_sync': datetime.now().isoformat(),
            'sync_type': 'full' if is_first_sync else 'incremental',
            'total_files_on_storage': len(all_files),
            'files_downloaded': 0,
            'files': {}
        }
        
        # Download files theo format
        if download_format == 'zip':
            # T·∫°o file zip v·ªõi timestamp
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            sync_type = 'full' if is_first_sync else 'incremental'
            zip_filename = f"supabase_storage_{sync_type}_{timestamp}.zip"
            zip_path = download_dir / zip_filename
            
            # T·∫°o th∆∞ m·ª•c temp
            temp_dir = download_dir / 'temp'
            temp_dir.mkdir(exist_ok=True)
            
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                
                # Download files trong folders
                for folder_name, files in folders_structure.items():
                    print(f"üìÅ Processing folder: {folder_name} ({len(files)} files)")
                    
                    folder_results = {
                        'name': folder_name,
                        'files': [],
                        'total_files': len(files),
                        'success_count': 0,
                        'error_count': 0
                    }
                    
                    for file_info in files:
                        temp_file_path = temp_dir / file_info['full_path']
                        result = download_single_file(file_info, temp_file_path)
                        
                        if result['success']:
                            # Th√™m v√†o zip v·ªõi ƒë√∫ng c·∫•u tr√∫c th∆∞ m·ª•c
                            zipf.write(temp_file_path, file_info['full_path'])
                            
                            folder_results['files'].append(result)
                            folder_results['success_count'] += 1
                            download_results['total_files'] += 1
                            download_results['total_size'] += result['size']
                            
                            # C·∫≠p nh·∫≠t sync state
                            current_sync_state['files'][file_info['full_path']] = {
                                'updated_at': file_info.get('updated_at'),
                                'size': file_info.get('size'),
                                'checksum': result['checksum']
                            }
                            
                            # X√≥a file temp
                            temp_file_path.unlink()
                            
                        else:
                            folder_results['error_count'] += 1
                            download_results['errors'].append(result)
                    
                    download_results['folders'][folder_name] = folder_results
                
                # Download root files
                if root_files:
                    print(f"üìÑ Processing root files: {len(root_files)} files")
                    
                    for file_info in root_files:
                        temp_file_path = temp_dir / file_info['name']
                        result = download_single_file(file_info, temp_file_path)
                        
                        if result['success']:
                            zipf.write(temp_file_path, file_info['name'])
                            
                            download_results['root_files'].append(result)
                            download_results['total_files'] += 1
                            download_results['total_size'] += result['size']
                            
                            # C·∫≠p nh·∫≠t sync state
                            current_sync_state['files'][file_info['full_path']] = {
                                'updated_at': file_info.get('updated_at'),
                                'size': file_info.get('size'),
                                'checksum': result['checksum']
                            }
                            
                            # X√≥a file temp
                            temp_file_path.unlink()
                            
                        else:
                            download_results['errors'].append(result)
                
                # Th√™m metadata file n·∫øu c·∫ßn
                if include_metadata:
                    metadata_content = {
                        'download_info': {
                            'timestamp': datetime.now().isoformat(),
                            'sync_type': current_sync_state['sync_type'],
                            'bucket': SUPABASE_BUCKET,
                            'total_folders': len(folders_on_storage),
                            'files_on_storage': len(all_files),
                            'files_downloaded': download_results['total_files'],
                            'files_skipped': download_results['skipped_files'],
                            'total_size': download_results['total_size'],
                            'total_size_human': format_file_size(download_results['total_size'])
                        },
                        'folders_structure': list(folders_on_storage),
                        'downloaded_folders': folders_structure,
                        'downloaded_root_files': root_files
                    }
                    
                    metadata_path = temp_dir / 'metadata.json'
                    with open(metadata_path, 'w', encoding='utf-8') as f:
                        json.dump(metadata_content, f, indent=2, ensure_ascii=False, default=str)
                    
                    zipf.write(metadata_path, 'metadata.json')
                    metadata_path.unlink()
            
            # X√≥a th∆∞ m·ª•c temp
            if temp_dir.exists():
                shutil.rmtree(temp_dir)
            
            download_results['zip_file'] = str(zip_path)
            download_results['zip_size'] = zip_path.stat().st_size
            download_results['zip_size_human'] = format_file_size(zip_path.stat().st_size)
            
        else:
            # Download th√†nh folders ri√™ng bi·ªát
            
            # Download files trong folders
            for folder_name, files in folders_structure.items():
                print(f"üìÅ Downloading folder: {folder_name} ({len(files)} files)")
                
                folder_path = download_dir / folder_name
                # Folder ƒë√£ ƒë∆∞·ª£c t·∫°o ·ªü tr√™n
                
                folder_results = {
                    'name': folder_name,
                    'path': str(folder_path),
                    'files': [],
                    'total_files': len(files),
                    'success_count': 0,
                    'error_count': 0
                }
                
                for file_info in files:
                    local_file_path = folder_path / file_info['name']
                    
                    result = download_single_file(file_info, local_file_path)
                    
                    if result['success']:
                        folder_results['files'].append(result)
                        folder_results['success_count'] += 1
                        download_results['total_files'] += 1
                        download_results['total_size'] += result['size']
                        
                        # C·∫≠p nh·∫≠t sync state
                        current_sync_state['files'][file_info['full_path']] = {
                            'updated_at': file_info.get('updated_at'),
                            'size': file_info.get('size'),
                            'checksum': result['checksum']
                        }
                    else:
                        folder_results['error_count'] += 1
                        download_results['errors'].append(result)
                
                download_results['folders'][folder_name] = folder_results
            
            # Download root files
            if root_files:
                print(f"üìÑ Downloading root files: {len(root_files)} files")
                
                for file_info in root_files:
                    local_file_path = download_dir / file_info['name']
                    
                    result = download_single_file(file_info, local_file_path)
                    
                    if result['success']:
                        download_results['root_files'].append(result)
                        download_results['total_files'] += 1
                        download_results['total_size'] += result['size']
                        
                        # C·∫≠p nh·∫≠t sync state
                        current_sync_state['files'][file_info['full_path']] = {
                            'updated_at': file_info.get('updated_at'),
                            'size': file_info.get('size'),
                            'checksum': result['checksum']
                        }
                    else:
                        download_results['errors'].append(result)
        
        # C·∫≠p nh·∫≠t sync state v·ªõi files t·ª´ previous state (files kh√¥ng thay ƒë·ªïi)
        if not is_first_sync:
            for file_path, file_data in previous_sync_state.get('files', {}).items():
                if file_path not in current_sync_state['files']:
                    # File n√†y kh√¥ng ƒë∆∞·ª£c download l·∫ßn n√†y (kh√¥ng thay ƒë·ªïi)
                    current_sync_state['files'][file_path] = file_data
        
        current_sync_state['files_downloaded'] = download_results['total_files']
        
        # L∆∞u sync state m·ªõi
        try:
            with open(sync_state_file, 'w', encoding='utf-8') as f:
                json.dump(current_sync_state, f, indent=2, ensure_ascii=False, default=str)
            print(f"üíæ ƒê√£ l∆∞u sync state t·∫°i: {sync_state_file}")
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ l∆∞u sync state: {e}")
        
        # T·∫°o metadata file n·∫øu c·∫ßn (cho folder mode)
        if include_metadata and download_format != 'zip':
            metadata_content = {
                'download_info': {
                    'timestamp': datetime.now().isoformat(),
                    'sync_type': current_sync_state['sync_type'],
                    'bucket': SUPABASE_BUCKET,
                    'total_folders': len(folders_on_storage),
                    'files_on_storage': len(all_files),
                    'files_downloaded': download_results['total_files'],
                    'files_skipped': download_results['skipped_files'],
                    'total_size': download_results['total_size'],
                    'total_size_human': format_file_size(download_results['total_size'])
                },
                'folders_structure': list(folders_on_storage),
                'download_results': download_results
            }
            
            metadata_path = download_dir / 'download_metadata.json'
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata_content, f, indent=2, ensure_ascii=False, default=str)
            
            download_results['metadata_file'] = str(metadata_path)
        
        # T√≠nh to√°n th·ªëng k√™ cu·ªëi
        success_count = download_results['total_files']
        error_count = len(download_results['errors'])
        skipped_count = download_results['skipped_files']
        
        print(f"‚úÖ {'Full' if is_first_sync else 'Incremental'} sync ho√†n th√†nh!")
        print(f"üìä Th·ªëng k√™:")
        print(f"   - T·ªïng folders: {len(folders_on_storage)}")
        print(f"   - Files tr√™n storage: {len(all_files)}")
        print(f"   - Files downloaded: {success_count}")
        print(f"   - Files skipped: {skipped_count}")
        print(f"   - Files l·ªói: {error_count}")
        print(f"   - T·ªïng dung l∆∞·ª£ng downloaded: {format_file_size(download_results['total_size'])}")
        
        return jsonify({
            'success': True,
            'message': f'{"Full" if is_first_sync else "Incremental"} sync ho√†n th√†nh: {success_count} files downloaded, {skipped_count} files skipped',
            'data': {
                'download_path': str(download_dir.absolute()),
                'format': download_format,
                'sync_type': current_sync_state['sync_type'],
                'statistics': {
                    'total_folders': len(folders_on_storage),
                    'files_on_storage': len(all_files),
                    'files_downloaded': success_count,
                    'files_skipped': skipped_count,
                    'files_failed': error_count,
                    'total_size': download_results['total_size'],
                    'total_size_human': format_file_size(download_results['total_size'])
                },
                'results': download_results
            }
        })
        
    except Exception as e:
        print(f"‚ùå L·ªói download folders: {str(e)}")
        import traceback
        traceback.print_exc()
        
        return jsonify({
            'success': False,
            'error': f'L·ªói download: {str(e)}'
        }), 500


# Helper function ƒë·ªÉ l·∫•y danh s√°ch t·∫•t c·∫£ folders t·ª´ storage
def get_all_folders_from_storage(supabase_client, bucket_name):
    """
    L·∫•y danh s√°ch t·∫•t c·∫£ folders t·ª´ Supabase Storage
    """
    try:
        # L·∫•y t·∫•t c·∫£ files
        result = supabase_client.storage.from_(bucket_name).list()
        
        folders = set()
        
        def extract_folders_recursive(items, current_path=""):
            for item in items:
                item_name = item.get('name', '')
                
                if current_path:
                    full_path = f"{current_path}/{item_name}"
                else:
                    full_path = item_name
                
                # N·∫øu l√† folder (kh√¥ng c√≥ metadata file info)
                if item.get('metadata') is None or 'size' not in item.get('metadata', {}):
                    folders.add(full_path)
                    
                    # Recursively list contents of this folder
                    try:
                        subfolder_result = supabase_client.storage.from_(bucket_name).list(full_path)
                        extract_folders_recursive(subfolder_result, full_path)
                    except:
                        pass  # Ignore errors when listing subfolders
                else:
                    # File - extract its parent folder
                    if '/' in full_path:
                        parent_folder = '/'.join(full_path.split('/')[:-1])
                        folders.add(parent_folder)
        
        extract_folders_recursive(result)
        
        return list(folders)
        
    except Exception as e:
        print(f"Error getting folders: {e}")
        return []


# API ƒë·ªÉ l·∫•y danh s√°ch folders
@app.route('/api/storage/folders', methods=['GET'])
def get_storage_folders():
    """
    API ƒë·ªÉ l·∫•y danh s√°ch t·∫•t c·∫£ folders trong Supabase Storage
    """
    try:
        if DEMO_MODE:
            return jsonify({
                'success': False,
                'error': 'T√≠nh nƒÉng n√†y kh√¥ng kh·∫£ d·ª•ng ·ªü Demo Mode'
            }), 400
        
        folders = get_all_folders_from_storage(supabase, SUPABASE_BUCKET)
        
        return jsonify({
            'success': True,
            'data': {
                'folders': sorted(folders),
                'total_folders': len(folders)
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'L·ªói l·∫•y danh s√°ch folders: {str(e)}'
        }), 500

# Thay th·∫ø 2 API functions n√†y trong file ch√≠nh

# API ƒë·ªÉ rename th∆∞ m·ª•c - FIXED VERSION
@app.route('/api/storage/rename-folder', methods=['POST'])
def rename_folder():
    """
    API ƒë·ªÉ ƒë·ªïi t√™n th∆∞ m·ª•c trong storage - Fixed version
    """
    try:
        if DEMO_MODE:
            return jsonify({
                'success': False,
                'error': 'T√≠nh nƒÉng n√†y kh√¥ng kh·∫£ d·ª•ng ·ªü Demo Mode'
            }), 400
        
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng c√≥ d·ªØ li·ªáu JSON'
            }), 400
            
        old_folder_name = data.get('old_folder_name', '').strip()
        new_folder_name = data.get('new_folder_name', '').strip()
        
        if not old_folder_name or not new_folder_name:
            return jsonify({
                'success': False,
                'error': 'T√™n th∆∞ m·ª•c c≈© v√† m·ªõi kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng'
            }), 400
        
        if old_folder_name == new_folder_name:
            return jsonify({
                'success': False,
                'error': 'T√™n th∆∞ m·ª•c m·ªõi ph·∫£i kh√°c v·ªõi t√™n c≈©'
            }), 400
        
        # Validate t√™n folder m·ªõi
        if '/' in new_folder_name or '\\' in new_folder_name:
            return jsonify({
                'success': False,
                'error': 'T√™n th∆∞ m·ª•c kh√¥ng ƒë∆∞·ª£c ch·ª©a k√Ω t·ª± / ho·∫∑c \\'
            }), 400
        
        print(f"üîÑ ƒêang rename folder '{old_folder_name}' ‚Üí '{new_folder_name}'...")
        
        # L·∫•y t·∫•t c·∫£ files trong folder c≈©
        all_files = get_all_storage_files(supabase, SUPABASE_BUCKET)
        folder_files = [f for f in all_files if f['folder'] == old_folder_name]
        
        if not folder_files:
            return jsonify({
                'success': False,
                'error': f'Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c "{old_folder_name}" ho·∫∑c th∆∞ m·ª•c r·ªóng'
            }), 404
        
        # Ki·ªÉm tra folder m·ªõi ƒë√£ t·ªìn t·∫°i ch∆∞a
        existing_new_folder = [f for f in all_files if f['folder'] == new_folder_name]
        if existing_new_folder:
            return jsonify({
                'success': False,
                'error': f'Th∆∞ m·ª•c "{new_folder_name}" ƒë√£ t·ªìn t·∫°i'
            }), 400
        
        # Th·ª±c hi·ªán copy + delete cho t·ª´ng file (v√¨ Supabase kh√¥ng c√≥ move tr·ª±c ti·∫øp)
        moved_files = []
        failed_files = []
        
        for file_info in folder_files:
            old_path = file_info['full_path']
            new_path = f"{new_folder_name}/{file_info['name']}"
            
            try:
                print(f"  üìÑ Moving: {old_path} ‚Üí {new_path}")
                
                # B∆∞·ªõc 1: Download file content
                download_response = supabase.storage.from_(SUPABASE_BUCKET).download(old_path)
                
                if not download_response:
                    failed_files.append({
                        'file': file_info['name'],
                        'error': 'Kh√¥ng th·ªÉ download file t·ª´ path c≈©'
                    })
                    continue
                
                # B∆∞·ªõc 2: Upload v·ªõi path m·ªõi
                upload_response = supabase.storage.from_(SUPABASE_BUCKET).upload(
                    path=new_path,
                    file=download_response,
                    file_options={
                        "content-type": file_info.get('metadata', {}).get('content-type', 'application/octet-stream')
                    }
                )
                
                # Ki·ªÉm tra upload th√†nh c√¥ng
                if hasattr(upload_response, 'error') and upload_response.error:
                    failed_files.append({
                        'file': file_info['name'],
                        'error': f'Upload failed: {upload_response.error}'
                    })
                    continue
                
                # B∆∞·ªõc 3: X√≥a file c≈©
                delete_response = supabase.storage.from_(SUPABASE_BUCKET).remove([old_path])
                
                # Ki·ªÉm tra x√≥a th√†nh c√¥ng
                if hasattr(delete_response, 'error') and delete_response.error:
                    print(f"‚ö†Ô∏è Warning: Couldn't delete old file {old_path}: {delete_response.error}")
                    # Kh√¥ng fail to√†n b·ªô operation v√¨ file m·ªõi ƒë√£ ƒë∆∞·ª£c t·∫°o
                
                moved_files.append({
                    'old_path': old_path,
                    'new_path': new_path,
                    'file_name': file_info['name']
                })
                    
            except Exception as e:
                failed_files.append({
                    'file': file_info['name'],
                    'error': str(e)
                })
                print(f"‚ùå Error moving {file_info['name']}: {str(e)}")
        
        if len(failed_files) == len(folder_files):
            # T·∫•t c·∫£ files ƒë·ªÅu fail
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng th·ªÉ move b·∫•t k·ª≥ file n√†o',
                'details': {
                    'failed_files': failed_files
                }
            }), 500
        
        success_count = len(moved_files)
        total_count = len(folder_files)
        
        print(f"‚úÖ Rename folder: {success_count}/{total_count} files moved successfully")
        
        # N·∫øu c√≥ m·ªôt s·ªë files fail nh∆∞ng kh√¥ng ph·∫£i t·∫•t c·∫£
        if failed_files:
            return jsonify({
                'success': True,
                'message': f'ƒê√£ ƒë·ªïi t√™n th∆∞ m·ª•c "{old_folder_name}" th√†nh "{new_folder_name}" ({success_count}/{total_count} files)',
                'warning': f'{len(failed_files)} files kh√¥ng th·ªÉ move',
                'data': {
                    'old_folder_name': old_folder_name,
                    'new_folder_name': new_folder_name,
                    'moved_files_count': success_count,
                    'failed_files_count': len(failed_files),
                    'moved_files': moved_files,
                    'failed_files': failed_files
                }
            })
        
        return jsonify({
            'success': True,
            'message': f'ƒê√£ ƒë·ªïi t√™n th∆∞ m·ª•c "{old_folder_name}" th√†nh "{new_folder_name}" th√†nh c√¥ng',
            'data': {
                'old_folder_name': old_folder_name,
                'new_folder_name': new_folder_name,
                'moved_files_count': success_count,
                'moved_files': moved_files
            }
        })
        
    except Exception as e:
        print(f"‚ùå L·ªói rename folder: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'success': False,
            'error': f'L·ªói rename folder: {str(e)}'
        }), 500


# API ƒë·ªÉ x√≥a th∆∞ m·ª•c - FIXED VERSION (S·ª≠ d·ª•ng POST thay v√¨ DELETE ƒë·ªÉ tr√°nh conflict)
@app.route('/api/storage/delete-folder', methods=['POST', 'DELETE'])
def delete_folder():
    """
    API ƒë·ªÉ x√≥a th∆∞ m·ª•c v√† t·∫•t c·∫£ files b√™n trong - Fixed version
    H·ªó tr·ª£ c·∫£ POST v√† DELETE methods
    """
    try:
        if DEMO_MODE:
            return jsonify({
                'success': False,
                'error': 'T√≠nh nƒÉng n√†y kh√¥ng kh·∫£ d·ª•ng ·ªü Demo Mode'
            }), 400
        
        # X·ª≠ l√Ω data t·ª´ request
        if request.method == 'POST':
            data = request.get_json()
        else:  # DELETE method
            data = request.get_json() if request.is_json else {}
            # N·∫øu kh√¥ng c√≥ JSON data, th·ª≠ l·∫•y t·ª´ query params
            if not data:
                folder_name = request.args.get('folder_name', '').strip()
                confirm_delete = request.args.get('confirm_delete', '').lower() == 'true'
                data = {
                    'folder_name': folder_name,
                    'confirm_delete': confirm_delete
                }
        
        if not data:
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng c√≥ d·ªØ li·ªáu request'
            }), 400
            
        folder_name = data.get('folder_name', '').strip()
        confirm_delete = data.get('confirm_delete', False)
        
        if not folder_name:
            return jsonify({
                'success': False,
                'error': 'T√™n th∆∞ m·ª•c kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng'
            }), 400
        
        if not confirm_delete:
            return jsonify({
                'success': False,
                'error': 'Vui l√≤ng x√°c nh·∫≠n x√≥a th∆∞ m·ª•c b·∫±ng c√°ch set confirm_delete = true'
            }), 400
        
        print(f"üóëÔ∏è ƒêang x√≥a folder '{folder_name}'...")
        
        # L·∫•y t·∫•t c·∫£ files trong folder
        all_files = get_all_storage_files(supabase, SUPABASE_BUCKET)
        folder_files = [f for f in all_files if f['folder'] == folder_name]
        
        if not folder_files:
            return jsonify({
                'success': False,
                'error': f'Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c "{folder_name}" ho·∫∑c th∆∞ m·ª•c r·ªóng'
            }), 404
        
        print(f"üìä T√¨m th·∫•y {len(folder_files)} files ƒë·ªÉ x√≥a")
        
        # Chu·∫©n b·ªã danh s√°ch paths ƒë·ªÉ x√≥a
        file_paths = [f['full_path'] for f in folder_files]
        
        # Th·ª±c hi·ªán x√≥a theo batch (Supabase c√≥ th·ªÉ x√≥a nhi·ªÅu files c√πng l√∫c)
        deleted_files = []
        failed_files = []
        
        # Chia nh·ªè th√†nh batches ƒë·ªÉ tr√°nh timeout (20 files/batch ƒë·ªÉ tƒÉng ƒë·ªô ·ªïn ƒë·ªãnh)
        batch_size = 20
        total_batches = (len(file_paths) + batch_size - 1) // batch_size
        
        for i in range(0, len(file_paths), batch_size):
            batch_paths = file_paths[i:i + batch_size]
            batch_files = folder_files[i:i + batch_size]
            current_batch = i // batch_size + 1
            
            try:
                print(f"üóÇÔ∏è X√≥a batch {current_batch}/{total_batches}: {len(batch_paths)} files")
                
                # X√≥a batch files
                delete_response = supabase.storage.from_(SUPABASE_BUCKET).remove(batch_paths)
                
                # Ki·ªÉm tra response (Supabase tr·∫£ v·ªÅ list ho·∫∑c c√≥ th·ªÉ c√≥ error)
                if hasattr(delete_response, 'error') and delete_response.error:
                    # N·∫øu batch fail, th·ª≠ x√≥a t·ª´ng file ri√™ng
                    print(f"‚ö†Ô∏è Batch delete failed: {delete_response.error}, trying individual files...")
                    
                    for j, file_path in enumerate(batch_paths):
                        try:
                            single_response = supabase.storage.from_(SUPABASE_BUCKET).remove([file_path])
                            
                            if hasattr(single_response, 'error') and single_response.error:
                                failed_files.append({
                                    'file': batch_files[j]['name'],
                                    'path': file_path,
                                    'error': str(single_response.error)
                                })
                            else:
                                deleted_files.append({
                                    'file': batch_files[j]['name'],
                                    'path': file_path,
                                    'size': batch_files[j]['size']
                                })
                        except Exception as e:
                            failed_files.append({
                                'file': batch_files[j]['name'],
                                'path': file_path,
                                'error': str(e)
                            })
                else:
                    # Batch delete th√†nh c√¥ng
                    for file_info in batch_files:
                        deleted_files.append({
                            'file': file_info['name'],
                            'path': file_info['full_path'],
                            'size': file_info['size']
                        })
                    
            except Exception as e:
                print(f"‚ùå Error deleting batch {current_batch}: {str(e)}")
                # N·∫øu c√≥ l·ªói v·ªõi batch, th·ª≠ x√≥a t·ª´ng file
                for j, file_path in enumerate(batch_paths):
                    try:
                        single_response = supabase.storage.from_(SUPABASE_BUCKET).remove([file_path])
                        
                        if hasattr(single_response, 'error') and single_response.error:
                            failed_files.append({
                                'file': batch_files[j]['name'],
                                'path': file_path,
                                'error': str(single_response.error)
                            })
                        else:
                            deleted_files.append({
                                'file': batch_files[j]['name'],
                                'path': file_path,
                                'size': batch_files[j]['size']
                            })
                    except Exception as inner_e:
                        failed_files.append({
                            'file': batch_files[j]['name'],
                            'path': file_path,
                            'error': str(inner_e)
                        })
        
        # T√≠nh to√°n k·∫øt qu·∫£
        success_count = len(deleted_files)
        failed_count = len(failed_files)
        total_count = len(folder_files)
        
        if success_count == 0:
            return jsonify({
                'success': False,
                'error': f'Kh√¥ng th·ªÉ x√≥a b·∫•t k·ª≥ file n√†o trong th∆∞ m·ª•c "{folder_name}"',
                'details': {
                    'failed_files': failed_files
                }
            }), 500
        
        # T√≠nh t·ªïng size ƒë√£ x√≥a
        total_deleted_size = sum(f['size'] for f in deleted_files)
        
        print(f"‚úÖ X√≥a folder ho√†n th√†nh: {success_count}/{total_count} files")
        
        # N·∫øu c√≥ m·ªôt s·ªë files fail
        if failed_files:
            return jsonify({
                'success': True,
                'message': f'ƒê√£ x√≥a th∆∞ m·ª•c "{folder_name}" ({success_count}/{total_count} files)',
                'warning': f'{failed_count} files kh√¥ng th·ªÉ x√≥a',
                'data': {
                    'folder_name': folder_name,
                    'deleted_files_count': success_count,
                    'failed_files_count': failed_count,
                    'total_deleted_size': total_deleted_size,
                    'total_deleted_size_human': format_file_size(total_deleted_size),
                    'deleted_files': deleted_files,
                    'failed_files': failed_files
                }
            })
        
        return jsonify({
            'success': True,
            'message': f'ƒê√£ x√≥a th∆∞ m·ª•c "{folder_name}" v√† t·∫•t c·∫£ {success_count} files',
            'data': {
                'folder_name': folder_name,
                'deleted_files_count': success_count,
                'total_deleted_size': total_deleted_size,
                'total_deleted_size_human': format_file_size(total_deleted_size),
                'deleted_files': deleted_files
            }
        })
        
    except Exception as e:
        print(f"‚ùå L·ªói delete folder: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'success': False,
            'error': f'L·ªói delete folder: {str(e)}'
        }), 500


# API ƒë·ªÉ test connection Supabase
@app.route('/api/storage/test-connection', methods=['GET'])
def test_supabase_connection():
    """
    API ƒë·ªÉ test k·∫øt n·ªëi Supabase
    """
    try:
        if DEMO_MODE:
            return jsonify({
                'success': True,
                'message': 'ƒêang ch·∫°y ·ªü Demo Mode',
                'data': {
                    'demo_mode': True,
                    'supabase_configured': False
                }
            })
        
        # Test b·∫±ng c√°ch list files
        test_result = supabase.storage.from_(SUPABASE_BUCKET).list(path="", options={"limit": 1})
        
        return jsonify({
            'success': True,
            'message': 'K·∫øt n·ªëi Supabase th√†nh c√¥ng',
            'data': {
                'demo_mode': False,
                'supabase_configured': True,
                'bucket_name': SUPABASE_BUCKET,
                'supabase_url': SUPABASE_URL,
                'test_result': 'OK'
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'L·ªói k·∫øt n·ªëi Supabase: {str(e)}',
            'data': {
                'demo_mode': DEMO_MODE,
                'supabase_configured': bool(SUPABASE_URL and SUPABASE_KEY),
                'bucket_name': SUPABASE_BUCKET
            }
        }), 500


# API ƒë·ªÉ t·∫°o th∆∞ m·ª•c m·ªõi (Bonus function)
@app.route('/api/storage/create-folder', methods=['POST'])
def create_folder():
    """
    API ƒë·ªÉ t·∫°o th∆∞ m·ª•c m·ªõi b·∫±ng c√°ch upload file placeholder
    """
    try:
        if DEMO_MODE:
            return jsonify({
                'success': False,
                'error': 'T√≠nh nƒÉng n√†y kh√¥ng kh·∫£ d·ª•ng ·ªü Demo Mode'
            }), 400
        
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng c√≥ d·ªØ li·ªáu JSON'
            }), 400
            
        folder_name = data.get('folder_name', '').strip()
        
        if not folder_name:
            return jsonify({
                'success': False,
                'error': 'T√™n th∆∞ m·ª•c kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng'
            }), 400
        
        # Validate t√™n folder
        if '/' in folder_name or '\\' in folder_name:
            return jsonify({
                'success': False,
                'error': 'T√™n th∆∞ m·ª•c kh√¥ng ƒë∆∞·ª£c ch·ª©a k√Ω t·ª± / ho·∫∑c \\'
            }), 400
        
        # Ki·ªÉm tra folder ƒë√£ t·ªìn t·∫°i ch∆∞a
        all_files = get_all_storage_files(supabase, SUPABASE_BUCKET)
        existing_folder = [f for f in all_files if f['folder'] == folder_name]
        
        if existing_folder:
            return jsonify({
                'success': False,
                'error': f'Th∆∞ m·ª•c "{folder_name}" ƒë√£ t·ªìn t·∫°i'
            }), 400
        
        # T·∫°o folder b·∫±ng c√°ch upload file .gitkeep
        placeholder_path = f"{folder_name}/.gitkeep"
        placeholder_content = "# This file keeps the folder structure\n"
        
        upload_response = supabase.storage.from_(SUPABASE_BUCKET).upload(
            path=placeholder_path,
            file=placeholder_content.encode('utf-8'),
            file_options={
                "content-type": "text/plain"
            }
        )
        
        if hasattr(upload_response, 'error') and upload_response.error:
            return jsonify({
                'success': False,
                'error': f'Kh√¥ng th·ªÉ t·∫°o th∆∞ m·ª•c: {upload_response.error}'
            }), 500
        
        return jsonify({
            'success': True,
            'message': f'ƒê√£ t·∫°o th∆∞ m·ª•c "{folder_name}" th√†nh c√¥ng',
            'data': {
                'folder_name': folder_name,
                'placeholder_file': '.gitkeep'
            }
        })
        
    except Exception as e:
        print(f"‚ùå L·ªói create folder: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'L·ªói create folder: {str(e)}'
        }), 500

# API ƒë·ªÉ l·∫•y th√¥ng tin chi ti·∫øt c·ªßa 1 th∆∞ m·ª•c
@app.route('/api/storage/folder-info/<folder_name>', methods=['GET'])
def get_folder_info(folder_name):
    """
    API ƒë·ªÉ l·∫•y th√¥ng tin chi ti·∫øt c·ªßa 1 th∆∞ m·ª•c
    """
    try:
        if DEMO_MODE:
            return jsonify({
                'success': False,
                'error': 'T√≠nh nƒÉng n√†y kh√¥ng kh·∫£ d·ª•ng ·ªü Demo Mode'
            }), 400
        
        folder_name = folder_name.strip()
        if not folder_name:
            return jsonify({
                'success': False,
                'error': 'T√™n th∆∞ m·ª•c kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng'
            }), 400
        
        print(f"üìÅ ƒêang l·∫•y th√¥ng tin folder '{folder_name}'...")
        
        # L·∫•y t·∫•t c·∫£ files trong folder
        all_files = get_all_storage_files(supabase, SUPABASE_BUCKET)
        folder_files = [f for f in all_files if f['folder'] == folder_name]
        
        if not folder_files:
            return jsonify({
                'success': False,
                'error': f'Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c "{folder_name}"'
            }), 404
        
        # T√≠nh to√°n th·ªëng k√™
        total_size = sum(f['size'] for f in folder_files)
        file_types = {}
        
        for file_info in folder_files:
            file_name = file_info['name']
            file_ext = file_name.split('.')[-1].lower() if '.' in file_name else 'no_extension'
            
            if file_ext not in file_types:
                file_types[file_ext] = {
                    'count': 0,
                    'total_size': 0
                }
            
            file_types[file_ext]['count'] += 1
            file_types[file_ext]['total_size'] += file_info['size']
        
        # Format file types
        for ext, info in file_types.items():
            info['total_size_human'] = format_file_size(info['total_size'])
        
        # S·∫Øp x·∫øp files theo size (l·ªõn nh·∫•t tr∆∞·ªõc)
        sorted_files = sorted(folder_files, key=lambda x: x['size'], reverse=True)
        
        return jsonify({
            'success': True,
            'data': {
                'folder_name': folder_name,
                'file_count': len(folder_files),
                'total_size': total_size,
                'total_size_human': format_file_size(total_size),
                'file_types': file_types,
                'files': sorted_files,
                'largest_files': sorted_files[:5],  # Top 5 files l·ªõn nh·∫•t
                'statistics': {
                    'total_file_types': len(file_types),
                    'average_file_size': total_size // len(folder_files) if folder_files else 0,
                    'average_file_size_human': format_file_size(total_size // len(folder_files)) if folder_files else '0 B'
                }
            }
        })
        
    except Exception as e:
        print(f"‚ùå L·ªói l·∫•y th√¥ng tin folder: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'L·ªói l·∫•y th√¥ng tin folder: {str(e)}'
        }), 500
      
@app.route('/api/submissions', methods=['GET'])
def get_submissions():
    try:
        # L·∫•y query parameters
        page = int(request.args.get('page', 1))
        limit = min(int(request.args.get('limit', 10)), 100)  # Max 100 records per page
        sort_by = request.args.get('sort_by', 'upload_time')
        sort_order = request.args.get('sort_order', 'desc').lower()
        folder_filter = request.args.get('folder', '').strip()
        search_query = request.args.get('search', '').strip()
        date_from = request.args.get('date_from', '').strip()
        date_to = request.args.get('date_to', '').strip()
        has_file_filter = request.args.get('has_file', '').strip().lower()
        
        # Validate parameters
        if page < 1:
            page = 1
        if limit < 1:
            limit = 10
        if sort_order not in ['asc', 'desc']:
            sort_order = 'desc'
        
        # Allowed sort fields
        allowed_sort_fields = ['upload_time', 'ho_ten', 'ten_de_tai', 'gio_quy_doi', 'folder_name', 'file_size']
        if sort_by not in allowed_sort_fields:
            sort_by = 'upload_time'
        
        # Handle demo mode
        if DEMO_MODE:
            # Generate demo data
            demo_submissions = []
            for i in range(1, 25):  # 24 demo records
                demo_submissions.append({
                    'id': i,
                    'ho_ten': f'Demo User {i}',
                    'ten_de_tai': f'Demo Project {i}',
                    'noi_cong_tac': f'Demo Company {i}',
                    'khoa_phong': f'Demo Department {i}',
                    'gio_quy_doi': round(i * 1.5, 2),
                    'minh_chung': f'Demo evidence {i}',
                    'ghi_chu': f'Demo note {i}',
                    'file_name': f'demo_file_{i}.pdf' if i % 2 == 0 else None,
                    'file_url': f'https://demo.example.com/demo_file_{i}.pdf' if i % 2 == 0 else None,
                    'file_size': i * 1024 if i % 2 == 0 else 0,
                    'folder_name': f'Demo Folder {(i % 5) + 1}' if i % 3 == 0 else None,
                    'upload_time': (datetime.datetime.now() - datetime.timedelta(days=i)).isoformat(),
                    'upload_ip': '127.0.0.1',
                    'storage_path': f'demo-folder-{(i % 5) + 1}/demo_file_{i}.pdf' if i % 2 == 0 else None
                })
            
            # Apply filters to demo data
            filtered_submissions = demo_submissions
            
            # Apply search filter
            if search_query:
                filtered_submissions = [
                    s for s in filtered_submissions 
                    if search_query.lower() in s['ho_ten'].lower() or 
                       search_query.lower() in s['ten_de_tai'].lower()
                ]
            
            # Apply folder filter
            if folder_filter:
                filtered_submissions = [
                    s for s in filtered_submissions 
                    if s['folder_name'] and folder_filter.lower() in s['folder_name'].lower()
                ]
            
            # Apply has_file filter
            if has_file_filter == 'true':
                filtered_submissions = [s for s in filtered_submissions if s['file_name']]
            elif has_file_filter == 'false':
                filtered_submissions = [s for s in filtered_submissions if not s['file_name']]
            
            # Apply sorting
            reverse_sort = sort_order == 'desc'
            filtered_submissions.sort(key=lambda x: x.get(sort_by, ''), reverse=reverse_sort)
            
            # Apply pagination
            offset = (page - 1) * limit
            paginated_submissions = filtered_submissions[offset:offset + limit]
            
            return jsonify({
                'success': True,
                'data': paginated_submissions,
                'pagination': {
                    'current_page': page,
                    'per_page': limit,
                    'total_records': len(filtered_submissions),
                    'total_pages': math.ceil(len(filtered_submissions) / limit),
                    'has_next': page < math.ceil(len(filtered_submissions) / limit),
                    'has_prev': page > 1
                },
                'filters': {
                    'folder': folder_filter,
                    'search': search_query,
                    'date_from': date_from,
                    'date_to': date_to,
                    'has_file': has_file_filter,
                    'sort_by': sort_by,
                    'sort_order': sort_order
                },
                'demo_mode': True
            })
        
        # Real Supabase query
        query = supabase.table('submissions').select('*')
        
        # Apply filters
        if folder_filter:
            query = query.ilike('folder_name', f'%{folder_filter}%')
        
        if search_query:
            # Search in multiple fields
            query = query.or_(f'ho_ten.ilike.%{search_query}%,ten_de_tai.ilike.%{search_query}%')
        
        if date_from:
            try:
                # Convert date format
                date_from_formatted = datetime.datetime.strptime(date_from, '%Y-%m-%d').isoformat()
                query = query.gte('upload_time', date_from_formatted)
            except ValueError:
                pass  # Invalid date format, ignore
        
        if date_to:
            try:
                # Convert date format and add end of day
                date_to_formatted = datetime.datetime.strptime(date_to, '%Y-%m-%d')
                date_to_formatted = date_to_formatted.replace(hour=23, minute=59, second=59).isoformat()
                query = query.lte('upload_time', date_to_formatted)
            except ValueError:
                pass  # Invalid date format, ignore
        
        if has_file_filter == 'true':
            query = query.not_.is_('file_name', 'null')
        elif has_file_filter == 'false':
            query = query.is_('file_name', 'null')
        
        # Get total count (before pagination)
        count_result = query.execute()
        total_records = len(count_result.data)
        
        # Apply sorting and pagination
        query = query.order(sort_by, desc=(sort_order == 'desc'))
        
        # Apply pagination
        offset = (page - 1) * limit
        query = query.limit(limit).offset(offset)
        
        # Execute query
        result = query.execute()
        
        # Process results
        submissions = []
        for submission in result.data:
            # Format file size
            file_size = submission.get('file_size', 0)
            if file_size:
                file_size_human = format_file_size(file_size)
            else:
                file_size_human = '0 B'
            
            # Format upload time
            upload_time = submission.get('upload_time')
            if upload_time:
                try:
                    upload_datetime = datetime.datetime.fromisoformat(upload_time.replace('Z', '+00:00'))
                    upload_time_formatted = upload_datetime.strftime('%d/%m/%Y %H:%M')
                except:
                    upload_time_formatted = upload_time
            else:
                upload_time_formatted = 'N/A'
            
            processed_submission = {
                **submission,
                'file_size_human': file_size_human,
                'upload_time_formatted': upload_time_formatted,
                'has_file': bool(submission.get('file_name')),
                'folder_display': submission.get('folder_name') or 'Kh√¥ng c√≥ th∆∞ m·ª•c'
            }
            
            submissions.append(processed_submission)
        
        # Calculate pagination info
        total_pages = math.ceil(total_records / limit)
        
        return jsonify({
            'success': True,
            'data': submissions,
            'pagination': {
                'current_page': page,
                'per_page': limit,
                'total_records': total_records,
                'total_pages': total_pages,
                'has_next': page < total_pages,
                'has_prev': page > 1,
                'offset': offset
            },
            'filters': {
                'folder': folder_filter,
                'search': search_query,
                'date_from': date_from,
                'date_to': date_to,
                'has_file': has_file_filter,
                'sort_by': sort_by,
                'sort_order': sort_order
            },
            'demo_mode': False
        })
        
    except Exception as e:
        print(f"Error in get_submissions: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'success': False,
            'error': f'L·ªói l·∫•y danh s√°ch submissions: {str(e)}'
        }), 500

@app.route('/api/submissions/<int:submission_id>', methods=['GET'])
def get_submission_detail(submission_id):
    """API ƒë·ªÉ l·∫•y chi ti·∫øt m·ªôt submission"""
    try:
        if DEMO_MODE:
            # Return demo data
            return jsonify({
                'success': True,
                'data': {
                    'id': submission_id,
                    'ho_ten': f'Demo User {submission_id}',
                    'ten_de_tai': f'Demo Project {submission_id}',
                    'noi_cong_tac': f'Demo Company {submission_id}',
                    'khoa_phong': f'Demo Department {submission_id}',
                    'gio_quy_doi': submission_id * 1.5,
                    'minh_chung': f'Demo evidence {submission_id}',
                    'ghi_chu': f'Demo note {submission_id}',
                    'file_name': f'demo_file_{submission_id}.pdf',
                    'file_url': f'https://demo.example.com/demo_file_{submission_id}.pdf',
                    'file_size': submission_id * 1024,
                    'file_size_human': format_file_size(submission_id * 1024),
                    'folder_name': f'Demo Folder {submission_id}',
                    'upload_time': datetime.datetime.now().isoformat(),
                    'upload_ip': '127.0.0.1',
                    'storage_path': f'demo-folder-{submission_id}/demo_file_{submission_id}.pdf'
                },
                'demo_mode': True
            })
        
        # Real query
        result = supabase.table('submissions').select('*').eq('id', submission_id).single().execute()
        
        if not result.data:
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng t√¨m th·∫•y submission'
            }), 404
        
        submission = result.data
        
        # Add formatted fields
        submission['file_size_human'] = format_file_size(submission.get('file_size', 0))
        submission['has_file'] = bool(submission.get('file_name'))
        submission['folder_display'] = submission.get('folder_name') or 'Kh√¥ng c√≥ th∆∞ m·ª•c'
        
        # Format upload time
        upload_time = submission.get('upload_time')
        if upload_time:
            try:
                upload_datetime = datetime.datetime.fromisoformat(upload_time.replace('Z', '+00:00'))
                submission['upload_time_formatted'] = upload_datetime.strftime('%d/%m/%Y %H:%M:%S')
            except:
                submission['upload_time_formatted'] = upload_time
        
        return jsonify({
            'success': True,
            'data': submission
        })
        
    except Exception as e:
        print(f"Error in get_submission_detail: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'L·ªói l·∫•y chi ti·∫øt submission: {str(e)}'
        }), 500

@app.route('/api/submissions/<int:submission_id>', methods=['PUT'])
def update_submission(submission_id):
    """API ƒë·ªÉ c·∫≠p nh·∫≠t m·ªôt submission"""
    try:
        if DEMO_MODE:
            return jsonify({
                'success': True,
                'message': 'Demo mode: C·∫≠p nh·∫≠t th√†nh c√¥ng (gi·∫£ l·∫≠p)',
                'demo_mode': True
            })
        
        # Get request data
        data = request.get_json()
        if not data:
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ c·∫≠p nh·∫≠t'
            }), 400
        
        # Allowed fields to update
        allowed_fields = [
            'ho_ten', 'ten_de_tai', 'noi_cong_tac', 'khoa_phong', 
            'gio_quy_doi', 'minh_chung', 'ghi_chu', 'folder_name'
        ]
        
        # Filter and validate data
        update_data = {}
        for field in allowed_fields:
            if field in data:
                value = data[field]
                if field == 'gio_quy_doi':
                    try:
                        value = float(value)
                    except (ValueError, TypeError):
                        value = 0.0
                elif field == 'folder_name' and value:
                    # Validate folder name
                    safe_folder_name = secure_folder_name(value)
                    if not safe_folder_name:
                        return jsonify({
                            'success': False,
                            'error': 'T√™n th∆∞ m·ª•c kh√¥ng h·ª£p l·ªá'
                        }), 400
                    value = safe_folder_name
                
                update_data[field] = value
        
        if not update_data:
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá ƒë·ªÉ c·∫≠p nh·∫≠t'
            }), 400
        
        # Update in database
        result = supabase.table('submissions').update(update_data).eq('id', submission_id).execute()
        
        if not result.data:
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng t√¨m th·∫•y ho·∫∑c kh√¥ng th·ªÉ c·∫≠p nh·∫≠t submission'
            }), 404
        
        return jsonify({
            'success': True,
            'message': 'C·∫≠p nh·∫≠t submission th√†nh c√¥ng',
            'data': result.data[0]
        })
        
    except Exception as e:
        print(f"Error in update_submission: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'L·ªói c·∫≠p nh·∫≠t submission: {str(e)}'
        }), 500

@app.route('/api/submissions/<int:submission_id>', methods=['DELETE'])
def delete_submission(submission_id):
    """API ƒë·ªÉ x√≥a m·ªôt submission"""
    try:
        if DEMO_MODE:
            return jsonify({
                'success': True,
                'message': 'Demo mode: X√≥a th√†nh c√¥ng (gi·∫£ l·∫≠p)',
                'demo_mode': True
            })
        
        # Get submission info first
        submission_result = supabase.table('submissions').select('*').eq('id', submission_id).single().execute()
        
        if not submission_result.data:
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng t√¨m th·∫•y submission'
            }), 404
        
        submission = submission_result.data
        storage_path = submission.get('storage_path')
        
        # Delete file from storage if exists
        if storage_path:
            try:
                delete_result = supabase.storage.from_(SUPABASE_BUCKET).remove([storage_path])
                print(f"File deleted from storage: {storage_path}")
            except Exception as storage_error:
                print(f"Error deleting file from storage: {str(storage_error)}")
                # Continue with database deletion even if file deletion fails
        
        # Delete from database
        delete_result = supabase.table('submissions').delete().eq('id', submission_id).execute()
        
        if not delete_result.data:
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng th·ªÉ x√≥a submission'
            }), 500
        
        return jsonify({
            'success': True,
            'message': 'X√≥a submission th√†nh c√¥ng',
            'deleted_file': storage_path
        })
        
    except Exception as e:
        print(f"Error in delete_submission: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'L·ªói x√≥a submission: {str(e)}'
        }), 500

@app.route('/api/submissions/stats', methods=['GET'])
def get_submissions_stats():
    """API ƒë·ªÉ l·∫•y th·ªëng k√™ submissions"""
    try:
        if DEMO_MODE:
            return jsonify({
                'success': True,
                'data': {
                    'total_submissions': 24,
                    'submissions_with_files': 12,
                    'submissions_without_files': 12,
                    'total_file_size': 24576,
                    'total_file_size_human': format_file_size(24576),
                    'total_folders': 5,
                    'avg_file_size': 2048,
                    'avg_file_size_human': format_file_size(2048),
                    'submissions_by_folder': {
                        'Demo Folder 1': 5,
                        'Demo Folder 2': 4,
                        'Demo Folder 3': 3,
                        'Kh√¥ng c√≥ th∆∞ m·ª•c': 12
                    },
                    'submissions_by_month': {
                        '2024-01': 8,
                        '2024-02': 10,
                        '2024-03': 6
                    },
                    'file_types': {
                        'pdf': 8,
                        'docx': 3,
                        'xlsx': 1
                    }
                },
                'demo_mode': True
            })
        
        # Real statistics
        result = supabase.table('submissions').select('*').execute()
        submissions = result.data
        
        total_submissions = len(submissions)
        submissions_with_files = len([s for s in submissions if s.get('file_name')])
        submissions_without_files = total_submissions - submissions_with_files
        
        # File size statistics
        file_sizes = [s.get('file_size', 0) for s in submissions if s.get('file_size')]
        total_file_size = sum(file_sizes)
        avg_file_size = total_file_size / len(file_sizes) if file_sizes else 0
        
        # Folder statistics
        folder_counts = {}
        for submission in submissions:
            folder = submission.get('folder_name') or 'Kh√¥ng c√≥ th∆∞ m·ª•c'
            folder_counts[folder] = folder_counts.get(folder, 0) + 1
        
        # Monthly statistics
        monthly_counts = {}
        for submission in submissions:
            upload_time = submission.get('upload_time')
            if upload_time:
                try:
                    date = datetime.datetime.fromisoformat(upload_time.replace('Z', '+00:00'))
                    month_key = date.strftime('%Y-%m')
                    monthly_counts[month_key] = monthly_counts.get(month_key, 0) + 1
                except:
                    pass
        
        # File type statistics
        file_type_counts = {}
        for submission in submissions:
            file_name = submission.get('file_name')
            if file_name and '.' in file_name:
                ext = file_name.split('.')[-1].lower()
                file_type_counts[ext] = file_type_counts.get(ext, 0) + 1
        
        return jsonify({
            'success': True,
            'data': {
                'total_submissions': total_submissions,
                'submissions_with_files': submissions_with_files,
                'submissions_without_files': submissions_without_files,
                'total_file_size': total_file_size,
                'total_file_size_human': format_file_size(total_file_size),
                'total_folders': len([f for f in folder_counts.keys() if f != 'Kh√¥ng c√≥ th∆∞ m·ª•c']),
                'avg_file_size': int(avg_file_size),
                'avg_file_size_human': format_file_size(avg_file_size),
                'submissions_by_folder': folder_counts,
                'submissions_by_month': monthly_counts,
                'file_types': file_type_counts
            }
        })
        
    except Exception as e:
        print(f"Error in get_submissions_stats: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'L·ªói l·∫•y th·ªëng k√™ submissions: {str(e)}'
        }), 500

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=int(os.environ.get('PORT', 5000)))